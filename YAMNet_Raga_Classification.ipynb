{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965fe1e1",
   "metadata": {},
   "source": [
    "# YAMNet Fine-tuning for Indian Classical Raga Classification\n",
    "\n",
    "This comprehensive notebook demonstrates how to fine-tune Google's YAMNet model for Indian classical raga classification using TensorFlow Hub and HuggingFace integration.\n",
    "\n",
    "## Overview\n",
    "\n",
    "YAMNet is a pre-trained deep neural network that can classify audio into 521 different categories. We'll leverage its powerful audio feature extraction capabilities and add a custom classification head for raga identification.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. üéµ Audio preprocessing for Indian classical music\n",
    "2. ü§ñ Transfer learning with pre-trained audio models\n",
    "3. üìä Advanced training techniques and data augmentation\n",
    "4. üìà Model evaluation and performance visualization\n",
    "5. üöÄ Model deployment to HuggingFace Hub\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Google Colab or local environment with GPU support\n",
    "- Google Drive with raga dataset\n",
    "- HuggingFace account for model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa5b2d",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and GPU Configuration\n",
    "\n",
    "Let's start by configuring TensorFlow for optimal GPU usage and checking available hardware resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure TensorFlow GPU settings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable GPU memory growth to avoid allocation errors\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ Found {len(gpus)} GPU(s) - Memory growth enabled\")\n",
    "        \n",
    "        # Get GPU details\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            details = tf.config.experimental.get_device_details(gpu)\n",
    "            print(f\"   GPU {i}: {details.get('device_name', 'Unknown')}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ùå GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU found - using CPU\")\n",
    "\n",
    "# Check TensorFlow version and backend\n",
    "print(f\"\\nüìä TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üîß Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "print(f\"üöÄ GPU Available: {tf.test.is_gpu_available()}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd385b",
   "metadata": {},
   "source": [
    "## 2. Library Installation and Imports\n",
    "\n",
    "Installing required packages and importing necessary modules for audio processing and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow-hub librosa soundfile huggingface_hub datasets transformers\n",
    "!pip install scikit-learn matplotlib seaborn plotly tqdm ipywidgets\n",
    "!pip install audiomentations resampy pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ed67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "# HuggingFace\n",
    "from huggingface_hub import HfApi, create_repo, upload_file\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3810adf6",
   "metadata": {},
   "source": [
    "## 3. Google Drive Integration and Dataset Loading\n",
    "\n",
    "Mounting Google Drive and loading the raga dataset from your specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Configure dataset paths\n",
    "DATASET_PATH = \"/content/drive/MyDrive/Raga_Dataset\"  # Update this path\n",
    "MODELS_PATH = \"/content/drive/MyDrive/YAMNet_Models\"\n",
    "OUTPUT_PATH = \"/content/drive/MyDrive/YAMNet_Output\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Dataset path: {DATASET_PATH}\")\n",
    "print(f\"üíæ Models path: {MODELS_PATH}\")\n",
    "print(f\"üìä Output path: {OUTPUT_PATH}\")\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚úÖ Dataset found at {DATASET_PATH}\")\n",
    "    \n",
    "    # List available raga folders\n",
    "    raga_folders = [f for f in os.listdir(DATASET_PATH) \n",
    "                   if os.path.isdir(os.path.join(DATASET_PATH, f))]\n",
    "    print(f\"üéµ Found {len(raga_folders)} raga classes: {raga_folders}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please update the DATASET_PATH variable with your correct path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db304f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_audio_files(dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Discover all audio files in the dataset and create a metadata DataFrame.\n",
    "    \"\"\"\n",
    "    audio_files = []\n",
    "    supported_formats = ['.wav', '.mp3', '.flac', '.m4a', '.ogg']\n",
    "    \n",
    "    print(\"üîç Discovering audio files...\")\n",
    "    \n",
    "    for raga_folder in tqdm(os.listdir(dataset_path)):\n",
    "        raga_path = os.path.join(dataset_path, raga_folder)\n",
    "        \n",
    "        if not os.path.isdir(raga_path):\n",
    "            continue\n",
    "            \n",
    "        for audio_file in os.listdir(raga_path):\n",
    "            file_path = os.path.join(raga_path, audio_file)\n",
    "            file_ext = os.path.splitext(audio_file)[1].lower()\n",
    "            \n",
    "            if file_ext in supported_formats:\n",
    "                try:\n",
    "                    # Get audio metadata\n",
    "                    info = sf.info(file_path)\n",
    "                    \n",
    "                    audio_files.append({\n",
    "                        'file_path': file_path,\n",
    "                        'filename': audio_file,\n",
    "                        'raga': raga_folder,\n",
    "                        'duration': info.duration,\n",
    "                        'sample_rate': info.samplerate,\n",
    "                        'channels': info.channels,\n",
    "                        'format': file_ext[1:],\n",
    "                        'file_size_mb': os.path.getsize(file_path) / (1024 * 1024)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Error reading {file_path}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(audio_files)\n",
    "    print(f\"‚úÖ Found {len(df)} audio files\")\n",
    "    return df\n",
    "\n",
    "# Discover audio files\n",
    "if 'raga_folders' in locals() and raga_folders:\n",
    "    metadata_df = discover_audio_files(DATASET_PATH)\n",
    "    \n",
    "    # Display dataset statistics\n",
    "    print(f\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"Total files: {len(metadata_df)}\")\n",
    "    print(f\"Total duration: {metadata_df['duration'].sum():.2f} seconds ({metadata_df['duration'].sum()/3600:.2f} hours)\")\n",
    "    print(f\"Average duration: {metadata_df['duration'].mean():.2f} seconds\")\n",
    "    print(f\"Total size: {metadata_df['file_size_mb'].sum():.2f} MB\")\n",
    "    \n",
    "    # Class distribution\n",
    "    print(f\"\\nüéµ Raga distribution:\")\n",
    "    raga_counts = metadata_df['raga'].value_counts()\n",
    "    for raga, count in raga_counts.items():\n",
    "        percentage = (count / len(metadata_df)) * 100\n",
    "        print(f\"   {raga}: {count} files ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(f\"\\nüìã Sample data:\")\n",
    "    print(metadata_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d5308",
   "metadata": {},
   "source": [
    "## 4. Audio Preprocessing Pipeline\n",
    "\n",
    "Creating functions to preprocess audio files for YAMNet compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAMNet configuration\n",
    "YAMNET_SAMPLE_RATE = 16000\n",
    "YAMNET_DURATION = 0.975  # YAMNet processes 0.975 seconds at a time\n",
    "YAMNET_SAMPLES = int(YAMNET_SAMPLE_RATE * YAMNET_DURATION)\n",
    "\n",
    "def load_and_preprocess_audio(file_path: str, \n",
    "                            target_sr: int = YAMNET_SAMPLE_RATE,\n",
    "                            duration: Optional[float] = None,\n",
    "                            offset: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load and preprocess audio file for YAMNet.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to audio file\n",
    "        target_sr: Target sample rate (16kHz for YAMNet)\n",
    "        duration: Duration to load (None for full file)\n",
    "        offset: Offset to start loading from\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed audio array\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, sr = librosa.load(file_path, sr=target_sr, duration=duration, offset=offset)\n",
    "        \n",
    "        # Normalize audio to [-1, 1]\n",
    "        audio = librosa.util.normalize(audio)\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading {file_path}: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def remove_silence(audio: np.ndarray, \n",
    "                   sr: int = YAMNET_SAMPLE_RATE,\n",
    "                   top_db: int = 20) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Remove silence from audio using librosa's onset detection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Trim silence from beginning and end\n",
    "        audio_trimmed, _ = librosa.effects.trim(audio, top_db=top_db)\n",
    "        \n",
    "        # Remove very short audio clips\n",
    "        if len(audio_trimmed) < sr * 0.1:  # Less than 0.1 seconds\n",
    "            return audio  # Return original if trimmed audio is too short\n",
    "            \n",
    "        return audio_trimmed\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error in silence removal: {e}\")\n",
    "        return audio\n",
    "\n",
    "def segment_audio(audio: np.ndarray, \n",
    "                  sr: int = YAMNET_SAMPLE_RATE,\n",
    "                  segment_duration: float = YAMNET_DURATION,\n",
    "                  overlap: float = 0.5) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Segment audio into fixed-length chunks for YAMNet processing.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio array\n",
    "        sr: Sample rate\n",
    "        segment_duration: Duration of each segment\n",
    "        overlap: Overlap between segments (0.0-1.0)\n",
    "    \n",
    "    Returns:\n",
    "        List of audio segments\n",
    "    \"\"\"\n",
    "    segment_samples = int(sr * segment_duration)\n",
    "    hop_samples = int(segment_samples * (1 - overlap))\n",
    "    \n",
    "    segments = []\n",
    "    start = 0\n",
    "    \n",
    "    while start + segment_samples <= len(audio):\n",
    "        segment = audio[start:start + segment_samples]\n",
    "        segments.append(segment)\n",
    "        start += hop_samples\n",
    "    \n",
    "    # Handle the last segment if there's remaining audio\n",
    "    if start < len(audio) and len(audio) - start > segment_samples * 0.5:\n",
    "        # Pad the last segment to required length\n",
    "        last_segment = audio[start:]\n",
    "        if len(last_segment) < segment_samples:\n",
    "            padding = segment_samples - len(last_segment)\n",
    "            last_segment = np.pad(last_segment, (0, padding), mode='constant')\n",
    "        segments.append(last_segment)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def preprocess_audio_file(file_path: str, \n",
    "                         max_segments: int = 10,\n",
    "                         remove_silence_flag: bool = True) -> Tuple[List[np.ndarray], str]:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for a single audio file.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (list of audio segments, error message if any)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        audio = load_and_preprocess_audio(file_path)\n",
    "        \n",
    "        if len(audio) == 0:\n",
    "            return [], \"Failed to load audio\"\n",
    "        \n",
    "        # Remove silence if requested\n",
    "        if remove_silence_flag:\n",
    "            audio = remove_silence(audio)\n",
    "        \n",
    "        # Segment audio\n",
    "        segments = segment_audio(audio)\n",
    "        \n",
    "        # Limit number of segments\n",
    "        if len(segments) > max_segments:\n",
    "            # Select segments from different parts of the audio\n",
    "            indices = np.linspace(0, len(segments)-1, max_segments, dtype=int)\n",
    "            segments = [segments[i] for i in indices]\n",
    "        \n",
    "        return segments, \"\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return [], str(e)\n",
    "\n",
    "# Test preprocessing with a sample file\n",
    "if 'metadata_df' in locals() and len(metadata_df) > 0:\n",
    "    sample_file = metadata_df.iloc[0]['file_path']\n",
    "    print(f\"üß™ Testing preprocessing with: {sample_file}\")\n",
    "    \n",
    "    segments, error = preprocess_audio_file(sample_file, max_segments=3)\n",
    "    \n",
    "    if error:\n",
    "        print(f\"‚ùå Error: {error}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Successfully preprocessed!\")\n",
    "        print(f\"   Generated {len(segments)} segments\")\n",
    "        print(f\"   Each segment shape: {segments[0].shape}\")\n",
    "        print(f\"   Sample rate: {YAMNET_SAMPLE_RATE} Hz\")\n",
    "        print(f\"   Duration per segment: {len(segments[0])/YAMNET_SAMPLE_RATE:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519cb4c",
   "metadata": {},
   "source": [
    "## 5. Dataset Preparation and Splitting\n",
    "\n",
    "Organizing the dataset into train/validation/test splits with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(metadata_df: pd.DataFrame, \n",
    "                   test_size: float = 0.2, \n",
    "                   val_size: float = 0.2,\n",
    "                   random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split dataset into train/validation/test sets with stratification.\n",
    "    \"\"\"\n",
    "    # Ensure we have enough samples per class\n",
    "    min_samples_per_class = metadata_df['raga'].value_counts().min()\n",
    "    print(f\"üìä Minimum samples per class: {min_samples_per_class}\")\n",
    "    \n",
    "    if min_samples_per_class < 3:\n",
    "        print(\"‚ö†Ô∏è  Warning: Some classes have very few samples. Consider collecting more data.\")\n",
    "    \n",
    "    # Create label encoder\n",
    "    le = LabelEncoder()\n",
    "    metadata_df['raga_encoded'] = le.fit_transform(metadata_df['raga'])\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    train_val, test = train_test_split(\n",
    "        metadata_df, \n",
    "        test_size=test_size, \n",
    "        stratify=metadata_df['raga_encoded'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    val_size_adjusted = val_size / (1 - test_size)  # Adjust for the reduced dataset\n",
    "    train, val = train_test_split(\n",
    "        train_val,\n",
    "        test_size=val_size_adjusted,\n",
    "        stratify=train_val['raga_encoded'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Dataset splits:\")\n",
    "    print(f\"   Train: {len(train)} files ({len(train)/len(metadata_df)*100:.1f}%)\")\n",
    "    print(f\"   Validation: {len(val)} files ({len(val)/len(metadata_df)*100:.1f}%)\")\n",
    "    print(f\"   Test: {len(test)} files ({len(test)/len(metadata_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check class distribution in each split\n",
    "    print(f\"\\\\nüéµ Class distribution:\")\n",
    "    for split_name, split_data in [('Train', train), ('Val', val), ('Test', test)]:\n",
    "        print(f\"   {split_name}:\")\n",
    "        for raga in le.classes_:\n",
    "            count = len(split_data[split_data['raga'] == raga])\n",
    "            print(f\"      {raga}: {count} files\")\n",
    "    \n",
    "    return train, val, test, le\n",
    "\n",
    "def create_data_generators(train_df: pd.DataFrame, \n",
    "                          val_df: pd.DataFrame, \n",
    "                          test_df: pd.DataFrame,\n",
    "                          batch_size: int = 32,\n",
    "                          max_segments_per_file: int = 5) -> Tuple:\n",
    "    \"\"\"\n",
    "    Create TensorFlow data generators for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def process_file_batch(file_paths, labels):\n",
    "        \"\"\"Process a batch of files and return segments with labels.\"\"\"\n",
    "        batch_segments = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for file_path, label in zip(file_paths, labels):\n",
    "            segments, error = preprocess_audio_file(\n",
    "                file_path.numpy().decode('utf-8'), \n",
    "                max_segments=max_segments_per_file\n",
    "            )\n",
    "            \n",
    "            if not error and segments:\n",
    "                for segment in segments:\n",
    "                    batch_segments.append(segment)\n",
    "                    batch_labels.append(label)\n",
    "        \n",
    "        if batch_segments:\n",
    "            return np.array(batch_segments), np.array(batch_labels)\n",
    "        else:\n",
    "            # Return empty arrays with correct shape\n",
    "            return np.empty((0, YAMNET_SAMPLES)), np.empty((0,))\n",
    "    \n",
    "    def create_tf_dataset(df, shuffle=True):\n",
    "        \"\"\"Create TensorFlow dataset from DataFrame.\"\"\"\n",
    "        file_paths = df['file_path'].values\n",
    "        labels = df['raga_encoded'].values\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "        \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(df))\n",
    "        \n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.map(\n",
    "            lambda paths, lbls: tf.py_function(\n",
    "                process_file_batch, \n",
    "                [paths, lbls], \n",
    "                [tf.float32, tf.int64]\n",
    "            ),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        \n",
    "        # Filter out empty batches\n",
    "        dataset = dataset.filter(lambda x, y: tf.shape(x)[0] > 0)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    train_dataset = create_tf_dataset(train_df, shuffle=True)\n",
    "    val_dataset = create_tf_dataset(val_df, shuffle=False)\n",
    "    test_dataset = create_tf_dataset(test_df, shuffle=False)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Prepare dataset splits\n",
    "if 'metadata_df' in locals() and len(metadata_df) > 0:\n",
    "    train_df, val_df, test_df, label_encoder = prepare_dataset(metadata_df)\n",
    "    \n",
    "    # Save label encoder\n",
    "    label_encoder_path = os.path.join(OUTPUT_PATH, 'label_encoder.pkl')\n",
    "    with open(label_encoder_path, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    print(f\"üíæ Label encoder saved to: {label_encoder_path}\")\n",
    "    print(f\"üè∑Ô∏è  Class labels: {list(label_encoder.classes_)}\")\n",
    "    \n",
    "    # Store number of classes for model building\n",
    "    NUM_CLASSES = len(label_encoder.classes_)\n",
    "    print(f\"üî¢ Number of classes: {NUM_CLASSES}\")\n",
    "else:\n",
    "    print(\"‚ùå No metadata available. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb348d",
   "metadata": {},
   "source": [
    "## 6. YAMNet Model Loading and Feature Extraction\n",
    "\n",
    "Loading the pre-trained YAMNet model from TensorFlow Hub and understanding its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAMNet model URL\n",
    "YAMNET_MODEL_URL = \"https://tfhub.dev/google/yamnet/1\"\n",
    "\n",
    "def load_yamnet_model():\n",
    "    \"\"\"\n",
    "    Load YAMNet model from TensorFlow Hub.\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Loading YAMNet model from TensorFlow Hub...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        yamnet_model = hub.load(YAMNET_MODEL_URL)\n",
    "        print(\"‚úÖ YAMNet model loaded successfully!\")\n",
    "        \n",
    "        # Test the model with a sample input\n",
    "        test_input = tf.random.normal([YAMNET_SAMPLES])\n",
    "        scores, embeddings, spectrogram = yamnet_model(test_input)\n",
    "        \n",
    "        print(f\"üìä Model output shapes:\")\n",
    "        print(f\"   Scores (classifications): {scores.shape}\")\n",
    "        print(f\"   Embeddings (features): {embeddings.shape}\")\n",
    "        print(f\"   Spectrogram: {spectrogram.shape}\")\n",
    "        \n",
    "        return yamnet_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading YAMNet: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_yamnet_embeddings(yamnet_model, audio_segments: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract YAMNet embeddings from audio segments.\n",
    "    \n",
    "    Args:\n",
    "        yamnet_model: Loaded YAMNet model\n",
    "        audio_segments: List of audio segments\n",
    "    \n",
    "    Returns:\n",
    "        Array of embeddings\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    \n",
    "    for segment in audio_segments:\n",
    "        # Ensure segment is the right length\n",
    "        if len(segment) != YAMNET_SAMPLES:\n",
    "            if len(segment) < YAMNET_SAMPLES:\n",
    "                # Pad with zeros\n",
    "                segment = np.pad(segment, (0, YAMNET_SAMPLES - len(segment)), mode='constant')\n",
    "            else:\n",
    "                # Truncate\n",
    "                segment = segment[:YAMNET_SAMPLES]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        segment_tensor = tf.convert_to_tensor(segment, dtype=tf.float32)\n",
    "        \n",
    "        # Extract features\n",
    "        _, embeddings, _ = yamnet_model(segment_tensor)\n",
    "        \n",
    "        # Average embeddings across time (YAMNet outputs multiple frames)\n",
    "        avg_embedding = tf.reduce_mean(embeddings, axis=0)\n",
    "        embeddings_list.append(avg_embedding.numpy())\n",
    "    \n",
    "    return np.array(embeddings_list)\n",
    "\n",
    "# Load YAMNet model\n",
    "yamnet_model = load_yamnet_model()\n",
    "\n",
    "if yamnet_model is not None:\n",
    "    # Test feature extraction with sample data\n",
    "    if 'metadata_df' in locals() and len(metadata_df) > 0:\n",
    "        print(\"\\\\nüß™ Testing feature extraction...\")\n",
    "        \n",
    "        sample_file = metadata_df.iloc[0]['file_path']\n",
    "        sample_segments, error = preprocess_audio_file(sample_file, max_segments=2)\n",
    "        \n",
    "        if not error and sample_segments:\n",
    "            sample_embeddings = extract_yamnet_embeddings(yamnet_model, sample_segments)\n",
    "            print(f\"‚úÖ Feature extraction test successful!\")\n",
    "            print(f\"   Input: {len(sample_segments)} audio segments\")\n",
    "            print(f\"   Output: {sample_embeddings.shape} embeddings\")\n",
    "            print(f\"   Embedding dimension: {sample_embeddings.shape[1]}\")\n",
    "            \n",
    "            # Store embedding dimension for model building\n",
    "            EMBEDDING_DIM = sample_embeddings.shape[1]\n",
    "        else:\n",
    "            print(f\"‚ùå Feature extraction test failed: {error}\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without YAMNet model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3af057",
   "metadata": {},
   "source": [
    "## 7. Custom Classification Head Architecture\n",
    "\n",
    "Building a custom neural network head on top of YAMNet embeddings for raga classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86740a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raga_classifier(embedding_dim: int, \n",
    "                          num_classes: int,\n",
    "                          hidden_units: List[int] = [512, 256, 128],\n",
    "                          dropout_rate: float = 0.3,\n",
    "                          l2_reg: float = 0.01) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Create a custom classification head for raga classification.\n",
    "    \n",
    "    Args:\n",
    "        embedding_dim: YAMNet embedding dimension (1024)\n",
    "        num_classes: Number of raga classes\n",
    "        hidden_units: List of hidden layer sizes\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "        l2_reg: L2 regularization strength\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer for YAMNet embeddings\n",
    "    inputs = tf.keras.Input(shape=(embedding_dim,), name='yamnet_embeddings')\n",
    "    \n",
    "    # Start with the embeddings\n",
    "    x = inputs\n",
    "    \n",
    "    # Add hidden layers with batch normalization and dropout\n",
    "    for i, units in enumerate(hidden_units):\n",
    "        x = tf.keras.layers.Dense(\n",
    "            units, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            name=f'dense_{i+1}'\n",
    "        )(x)\n",
    "        \n",
    "        x = tf.keras.layers.BatchNormalization(name=f'batch_norm_{i+1}')(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate, name=f'dropout_{i+1}')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        num_classes, \n",
    "        activation='softmax',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "        name='raga_predictions'\n",
    "    )(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='RagaClassifier')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_end_to_end_model(yamnet_model, \n",
    "                           classifier_model,\n",
    "                           trainable_yamnet: bool = False) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Create an end-to-end model combining YAMNet and the classifier.\n",
    "    \n",
    "    Args:\n",
    "        yamnet_model: Pre-trained YAMNet model\n",
    "        classifier_model: Custom classification head\n",
    "        trainable_yamnet: Whether to fine-tune YAMNet weights\n",
    "    \n",
    "    Returns:\n",
    "        End-to-end Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Audio input\n",
    "    audio_input = tf.keras.Input(shape=(YAMNET_SAMPLES,), name='audio_input')\n",
    "    \n",
    "    # YAMNet feature extraction\n",
    "    # We only need the embeddings (index 1) from YAMNet output\n",
    "    yamnet_outputs = yamnet_model(audio_input)\n",
    "    embeddings = yamnet_outputs[1]  # Shape: (num_frames, 1024)\n",
    "    \n",
    "    # Average embeddings across time frames\n",
    "    avg_embeddings = tf.reduce_mean(embeddings, axis=0, keepdims=True)\n",
    "    \n",
    "    # Classification head\n",
    "    predictions = classifier_model(avg_embeddings)\n",
    "    \n",
    "    # Create end-to-end model\n",
    "    model = tf.keras.Model(inputs=audio_input, outputs=predictions, name='YAMNet_RagaClassifier')\n",
    "    \n",
    "    # Set YAMNet trainability\n",
    "    # Note: YAMNet is a hub.KerasLayer, so we need to handle it differently\n",
    "    if not trainable_yamnet:\n",
    "        # Freeze YAMNet weights for transfer learning\n",
    "        model.layers[1].trainable = False\n",
    "        print(\"üîí YAMNet weights frozen for transfer learning\")\n",
    "    else:\n",
    "        model.layers[1].trainable = True\n",
    "        print(\"üîì YAMNet weights trainable for fine-tuning\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create models\n",
    "if 'EMBEDDING_DIM' in locals() and 'NUM_CLASSES' in locals():\n",
    "    print(\"üèóÔ∏è  Building raga classification model...\")\n",
    "    \n",
    "    # Create classifier head\n",
    "    classifier = create_raga_classifier(\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        hidden_units=[512, 256, 128],\n",
    "        dropout_rate=0.3,\n",
    "        l2_reg=0.01\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Classification head created!\")\n",
    "    print(classifier.summary())\n",
    "    \n",
    "    # Create end-to-end model\n",
    "    model = create_end_to_end_model(\n",
    "        yamnet_model=yamnet_model,\n",
    "        classifier_model=classifier,\n",
    "        trainable_yamnet=False  # Start with frozen YAMNet\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\n‚úÖ End-to-end model created!\")\n",
    "    print(f\"üìä Model summary:\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Save model architecture\n",
    "    model_config = {\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'hidden_units': [512, 256, 128],\n",
    "        'dropout_rate': 0.3,\n",
    "        'l2_reg': 0.01,\n",
    "        'yamnet_url': YAMNET_MODEL_URL\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(OUTPUT_PATH, 'model_config.json')\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(model_config, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Model configuration saved to: {config_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Missing required variables. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836b21a",
   "metadata": {},
   "source": [
    "## 8. Data Augmentation and Generators\n",
    "\n",
    "Implementing audio augmentation techniques and creating efficient data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82542832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation configuration\n",
    "AUGMENTATION_CONFIG = {\n",
    "    'time_stretch_rate': [0.8, 1.2],\n",
    "    'pitch_shift_semitones': [-2, 2],\n",
    "    'noise_level': 0.005,\n",
    "    'augmentation_probability': 0.3\n",
    "}\n",
    "\n",
    "def create_augmentation_pipeline():\n",
    "    \"\"\"\n",
    "    Create audio augmentation pipeline using audiomentations.\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=AUGMENTATION_CONFIG['noise_level'], p=0.3),\n",
    "        TimeStretch(min_rate=AUGMENTATION_CONFIG['time_stretch_rate'][0], \n",
    "                   max_rate=AUGMENTATION_CONFIG['time_stretch_rate'][1], p=0.3),\n",
    "        PitchShift(min_semitones=AUGMENTATION_CONFIG['pitch_shift_semitones'][0],\n",
    "                  max_semitones=AUGMENTATION_CONFIG['pitch_shift_semitones'][1], p=0.3),\n",
    "        Shift(min_fraction=-0.5, max_fraction=0.5, p=0.3)\n",
    "    ])\n",
    "\n",
    "def augment_audio_segment(audio: np.ndarray, augment_pipeline, apply_augmentation: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply augmentation to an audio segment.\n",
    "    \"\"\"\n",
    "    if apply_augmentation and np.random.random() < AUGMENTATION_CONFIG['augmentation_probability']:\n",
    "        try:\n",
    "            augmented = augment_pipeline(samples=audio, sample_rate=YAMNET_SAMPLE_RATE)\n",
    "            return augmented\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Augmentation failed: {e}\")\n",
    "            return audio\n",
    "    return audio\n",
    "\n",
    "def create_efficient_data_generator(df: pd.DataFrame, \n",
    "                                  batch_size: int = 32,\n",
    "                                  max_segments_per_file: int = 5,\n",
    "                                  shuffle: bool = True,\n",
    "                                  augment: bool = True) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Create an efficient TensorFlow data generator with augmentation.\n",
    "    \"\"\"\n",
    "    augment_pipeline = create_augmentation_pipeline() if augment else None\n",
    "    \n",
    "    def generator():\n",
    "        \"\"\"Python generator function.\"\"\"\n",
    "        indices = np.arange(len(df))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        for idx in indices:\n",
    "            row = df.iloc[idx]\n",
    "            file_path = row['file_path']\n",
    "            label = row['raga_encoded']\n",
    "            \n",
    "            # Load and preprocess audio\n",
    "            segments, error = preprocess_audio_file(file_path, max_segments=max_segments_per_file)\n",
    "            \n",
    "            if not error and segments:\n",
    "                for segment in segments:\n",
    "                    # Apply augmentation if requested\n",
    "                    if augment and augment_pipeline:\n",
    "                        segment = augment_audio_segment(segment, augment_pipeline, apply_augmentation=True)\n",
    "                    \n",
    "                    # Ensure segment is correct length\n",
    "                    if len(segment) != YAMNET_SAMPLES:\n",
    "                        if len(segment) < YAMNET_SAMPLES:\n",
    "                            segment = np.pad(segment, (0, YAMNET_SAMPLES - len(segment)), mode='constant')\n",
    "                        else:\n",
    "                            segment = segment[:YAMNET_SAMPLES]\n",
    "                    \n",
    "                    yield segment.astype(np.float32), label\n",
    "    \n",
    "    # Create TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(YAMNET_SAMPLES,), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Batch and optimize\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def calculate_class_weights(train_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate class weights for handling imbalanced datasets.\n",
    "    \"\"\"\n",
    "    classes = train_df['raga_encoded'].values\n",
    "    unique_classes = np.unique(classes)\n",
    "    \n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=unique_classes,\n",
    "        y=classes\n",
    "    )\n",
    "    \n",
    "    class_weight_dict = dict(zip(unique_classes, weights))\n",
    "    \n",
    "    print(\"‚öñÔ∏è  Class weights for balanced training:\")\n",
    "    for class_idx, weight in class_weight_dict.items():\n",
    "        raga_name = label_encoder.inverse_transform([class_idx])[0]\n",
    "        print(f\"   {raga_name}: {weight:.3f}\")\n",
    "    \n",
    "    return class_weight_dict\n",
    "\n",
    "# Create data generators\n",
    "if all(var in locals() for var in ['train_df', 'val_df', 'test_df']):\n",
    "    print(\"üîÑ Creating data generators...\")\n",
    "    \n",
    "    # Training configuration\n",
    "    BATCH_SIZE = 16  # Adjust based on your GPU memory\n",
    "    MAX_SEGMENTS_PER_FILE = 8\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = create_efficient_data_generator(\n",
    "        train_df, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_segments_per_file=MAX_SEGMENTS_PER_FILE,\n",
    "        shuffle=True,\n",
    "        augment=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = create_efficient_data_generator(\n",
    "        val_df,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_segments_per_file=MAX_SEGMENTS_PER_FILE,\n",
    "        shuffle=False,\n",
    "        augment=False\n",
    "    )\n",
    "    \n",
    "    test_dataset = create_efficient_data_generator(\n",
    "        test_df,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_segments_per_file=MAX_SEGMENTS_PER_FILE,\n",
    "        shuffle=False,\n",
    "        augment=False\n",
    "    )\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = calculate_class_weights(train_df)\n",
    "    \n",
    "    print(\"‚úÖ Data generators created successfully!\")\n",
    "    print(f\"üìä Configuration:\")\n",
    "    print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"   Max segments per file: {MAX_SEGMENTS_PER_FILE}\")\n",
    "    print(f\"   Augmentation: Enabled for training\")\n",
    "    \n",
    "    # Test the generators\n",
    "    print(\"\\\\nüß™ Testing data generators...\")\n",
    "    for batch_audio, batch_labels in train_dataset.take(1):\n",
    "        print(f\"   Train batch: {batch_audio.shape} audio, {batch_labels.shape} labels\")\n",
    "        break\n",
    "    \n",
    "    for batch_audio, batch_labels in val_dataset.take(1):\n",
    "        print(f\"   Val batch: {batch_audio.shape} audio, {batch_labels.shape} labels\")\n",
    "        break\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Missing dataset splits. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b9eda",
   "metadata": {},
   "source": [
    "## 9. Training Configuration and Callbacks\n",
    "\n",
    "Setting up optimizer, loss function, learning rate scheduling, and training callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd498a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'initial_learning_rate': 0.001,\n",
    "    'min_learning_rate': 1e-7,\n",
    "    'epochs': 50,\n",
    "    'patience': 10,\n",
    "    'reduce_lr_patience': 5,\n",
    "    'reduce_lr_factor': 0.5,\n",
    "    'monitor_metric': 'val_accuracy'\n",
    "}\n",
    "\n",
    "def create_learning_rate_schedule():\n",
    "    \"\"\"\n",
    "    Create learning rate schedule with exponential decay.\n",
    "    \"\"\"\n",
    "    initial_learning_rate = TRAINING_CONFIG['initial_learning_rate']\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.96\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    return lr_schedule\n",
    "\n",
    "def setup_callbacks(model_checkpoint_path: str, log_dir: str):\n",
    "    \"\"\"\n",
    "    Setup training callbacks for monitoring and control.\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "    \n",
    "    # Model checkpoint - save best model\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_checkpoint_path,\n",
    "        monitor=TRAINING_CONFIG['monitor_metric'],\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(checkpoint_callback)\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=TRAINING_CONFIG['monitor_metric'],\n",
    "        patience=TRAINING_CONFIG['patience'],\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stopping)\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=TRAINING_CONFIG['monitor_metric'],\n",
    "        factor=TRAINING_CONFIG['reduce_lr_factor'],\n",
    "        patience=TRAINING_CONFIG['reduce_lr_patience'],\n",
    "        min_lr=TRAINING_CONFIG['min_learning_rate'],\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(reduce_lr)\n",
    "    \n",
    "    # TensorBoard logging\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        update_freq='epoch'\n",
    "    )\n",
    "    callbacks.append(tensorboard)\n",
    "    \n",
    "    # Custom callback for live plotting\n",
    "    class LivePlotCallback(tf.keras.callbacks.Callback):\n",
    "        def __init__(self):\n",
    "            self.losses = []\n",
    "            self.val_losses = []\n",
    "            self.accuracies = []\n",
    "            self.val_accuracies = []\n",
    "            \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            self.accuracies.append(logs.get('accuracy'))\n",
    "            self.val_accuracies.append(logs.get('val_accuracy'))\n",
    "            \n",
    "            # Clear output and plot\n",
    "            if epoch % 5 == 0 or epoch == 0:  # Plot every 5 epochs\n",
    "                self.plot_training_progress()\n",
    "        \n",
    "        def plot_training_progress(self):\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Loss plot\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(self.losses, label='Training Loss', color='blue')\n",
    "            plt.plot(self.val_losses, label='Validation Loss', color='red')\n",
    "            plt.title('Model Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Accuracy plot\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(self.accuracies, label='Training Accuracy', color='blue')\n",
    "            plt.plot(self.val_accuracies, label='Validation Accuracy', color='red')\n",
    "            plt.title('Model Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    live_plot = LivePlotCallback()\n",
    "    callbacks.append(live_plot)\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "def compile_model(model, learning_rate=None):\n",
    "    \"\"\"\n",
    "    Compile the model with optimizer, loss, and metrics.\n",
    "    \"\"\"\n",
    "    if learning_rate is None:\n",
    "        learning_rate = create_learning_rate_schedule()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = [\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.SparseCategoricalCrossentropy(name='crossentropy'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='top_3_accuracy')\n",
    "    ]\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Model compiled successfully!\")\n",
    "    print(f\"üìä Optimizer: Adam with learning rate schedule\")\n",
    "    print(f\"üìä Loss: Sparse Categorical Crossentropy\")\n",
    "    print(f\"üìä Metrics: Accuracy, Top-3 Accuracy, Crossentropy\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup training environment\n",
    "if 'model' in locals():\n",
    "    print(\"üîß Setting up training environment...\")\n",
    "    \n",
    "    # Create directories\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_name = f\"yamnet_raga_classifier_{timestamp}\"\n",
    "    \n",
    "    experiment_dir = os.path.join(OUTPUT_PATH, experiment_name)\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "    \n",
    "    model_checkpoint_path = os.path.join(experiment_dir, \"best_model.h5\")\n",
    "    log_dir = os.path.join(experiment_dir, \"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Experiment directory: {experiment_dir}\")\n",
    "    print(f\"üíæ Model checkpoint: {model_checkpoint_path}\")\n",
    "    print(f\"üìä TensorBoard logs: {log_dir}\")\n",
    "    \n",
    "    # Compile model\n",
    "    model = compile_model(model)\n",
    "    \n",
    "    # Setup callbacks\n",
    "    callbacks = setup_callbacks(model_checkpoint_path, log_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Training setup complete!\")\n",
    "    print(f\"üìã Callbacks configured: {len(callbacks)} callbacks\")\n",
    "    print(f\"üéØ Training configuration:\")\n",
    "    for key, value in TRAINING_CONFIG.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Model not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423660ee",
   "metadata": {},
   "source": [
    "## 10. Model Training Loop\n",
    "\n",
    "Executing the training process with proper validation monitoring and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236860ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate steps per epoch\n",
    "def calculate_dataset_size(dataset, max_batches=100):\n",
    "    \"\"\"\n",
    "    Estimate dataset size by sampling batches.\n",
    "    \"\"\"\n",
    "    total_samples = 0\n",
    "    for i, (batch_audio, batch_labels) in enumerate(dataset.take(max_batches)):\n",
    "        total_samples += len(batch_labels)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"   Processed {i+1} batches, {total_samples} samples so far...\")\n",
    "    return total_samples\n",
    "\n",
    "# Calculate dataset sizes\n",
    "if all(var in locals() for var in ['train_dataset', 'val_dataset', 'model', 'callbacks']):\n",
    "    print(\"üìä Calculating dataset sizes...\")\n",
    "    \n",
    "    # This might take a moment for large datasets\n",
    "    train_size = calculate_dataset_size(train_dataset, max_batches=50)\n",
    "    val_size = calculate_dataset_size(val_dataset, max_batches=20)\n",
    "    \n",
    "    steps_per_epoch = max(1, train_size // BATCH_SIZE)\n",
    "    validation_steps = max(1, val_size // BATCH_SIZE)\n",
    "    \n",
    "    print(f\"üìä Training dataset: ~{train_size} samples, {steps_per_epoch} steps per epoch\")\n",
    "    print(f\"üìä Validation dataset: ~{val_size} samples, {validation_steps} validation steps\")\n",
    "    \n",
    "    # Start training\n",
    "    print(f\"\\\\nüöÄ Starting training for {TRAINING_CONFIG['epochs']} epochs...\")\n",
    "    print(f\"üíæ Best model will be saved to: {model_checkpoint_path}\")\n",
    "    print(f\"üìä Monitor TensorBoard: tensorboard --logdir {log_dir}\")\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=TRAINING_CONFIG['epochs'],\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=validation_steps,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Training completed successfully!\")\n",
    "        \n",
    "        # Save training history\n",
    "        history_path = os.path.join(experiment_dir, \"training_history.json\")\n",
    "        \n",
    "        # Convert numpy arrays to lists for JSON serialization\n",
    "        history_dict = {}\n",
    "        for key, values in history.history.items():\n",
    "            history_dict[key] = [float(v) for v in values]\n",
    "        \n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history_dict, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Training history saved to: {history_path}\")\n",
    "        \n",
    "        # Load best model\n",
    "        if os.path.exists(model_checkpoint_path):\n",
    "            print(f\"üì• Loading best model from: {model_checkpoint_path}\")\n",
    "            best_model = tf.keras.models.load_model(model_checkpoint_path)\n",
    "            print(\"‚úÖ Best model loaded successfully!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Using final model (checkpoint not found)\")\n",
    "            best_model = model\n",
    "        \n",
    "        # Final training summary\n",
    "        print(f\"\\\\nüìä Training Summary:\")\n",
    "        print(f\"   Total epochs: {len(history.history['loss'])}\")\n",
    "        print(f\"   Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "        print(f\"   Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "        print(f\"   Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Missing required components. Please run all previous cells first.\")\n",
    "    missing = []\n",
    "    if 'train_dataset' not in locals():\n",
    "        missing.append('train_dataset')\n",
    "    if 'val_dataset' not in locals():\n",
    "        missing.append('val_dataset')\n",
    "    if 'model' not in locals():\n",
    "        missing.append('model')\n",
    "    if 'callbacks' not in locals():\n",
    "        missing.append('callbacks')\n",
    "    print(f\"Missing: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9777b9",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation and Metrics\n",
    "\n",
    "Evaluating the trained model on test data and calculating comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9acdecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataset, label_encoder, class_names=None):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with metrics and analysis.\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Evaluating model on test data...\")\n",
    "    \n",
    "    # Collect predictions and true labels\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    for batch_audio, batch_labels in tqdm(test_dataset, desc=\"Evaluating\"):\n",
    "        # Get predictions\n",
    "        predictions = model.predict(batch_audio, verbose=0)\n",
    "        \n",
    "        # Store results\n",
    "        y_true.extend(batch_labels.numpy())\n",
    "        y_pred.extend(np.argmax(predictions, axis=1))\n",
    "        y_pred_proba.extend(predictions)\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    \n",
    "    # Get class names\n",
    "    if class_names is None:\n",
    "        class_names = label_encoder.classes_\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=class_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    \n",
    "    # Top-k accuracy\n",
    "    top_3_accuracy = 0\n",
    "    for i, true_label in enumerate(y_true):\n",
    "        top_3_preds = np.argsort(y_pred_proba[i])[-3:]\n",
    "        if true_label in top_3_preds:\n",
    "            top_3_accuracy += 1\n",
    "    top_3_accuracy /= len(y_true)\n",
    "    \n",
    "    print(f\"‚úÖ Evaluation completed!\")\n",
    "    print(f\"üìä Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"üìä Top-3 Accuracy: {top_3_accuracy:.4f}\")\n",
    "    print(f\"üìä Macro F1-Score: {report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"üìä Weighted F1-Score: {report['weighted avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    # Per-class performance\n",
    "    print(f\"\\\\nüéµ Per-class Performance:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        precision = report[class_name]['precision']\n",
    "        recall = report[class_name]['recall']\n",
    "        f1 = report[class_name]['f1-score']\n",
    "        class_accuracy = per_class_accuracy[i]\n",
    "        \n",
    "        print(f\"   {class_name}:\")\n",
    "        print(f\"      Accuracy: {class_accuracy:.4f}\")\n",
    "        print(f\"      Precision: {precision:.4f}\")\n",
    "        print(f\"      Recall: {recall:.4f}\")\n",
    "        print(f\"      F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'top_3_accuracy': top_3_accuracy,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'per_class_accuracy': per_class_accuracy,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'class_names': class_names\n",
    "    }\n",
    "\n",
    "def analyze_misclassifications(evaluation_results, top_n=5):\n",
    "    \"\"\"\n",
    "    Analyze the most common misclassifications.\n",
    "    \"\"\"\n",
    "    y_true = evaluation_results['y_true']\n",
    "    y_pred = evaluation_results['y_pred']\n",
    "    class_names = evaluation_results['class_names']\n",
    "    cm = evaluation_results['confusion_matrix']\n",
    "    \n",
    "    print(f\"üîç Analyzing misclassifications...\")\n",
    "    \n",
    "    # Find most common misclassifications\n",
    "    misclassifications = []\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                misclassifications.append({\n",
    "                    'true_class': class_names[i],\n",
    "                    'predicted_class': class_names[j],\n",
    "                    'count': cm[i, j],\n",
    "                    'percentage': cm[i, j] / cm[i].sum() * 100\n",
    "                })\n",
    "    \n",
    "    # Sort by count\n",
    "    misclassifications.sort(key=lambda x: x['count'], reverse=True)\n",
    "    \n",
    "    print(f\"\\\\n‚ùå Top {top_n} Misclassifications:\")\n",
    "    for i, misc in enumerate(misclassifications[:top_n]):\n",
    "        print(f\"   {i+1}. {misc['true_class']} ‚Üí {misc['predicted_class']}: \"\n",
    "              f\"{misc['count']} times ({misc['percentage']:.1f}%)\")\n",
    "    \n",
    "    return misclassifications\n",
    "\n",
    "def save_evaluation_results(evaluation_results, save_path):\n",
    "    \"\"\"\n",
    "    Save evaluation results to file.\n",
    "    \"\"\"\n",
    "    # Prepare data for saving (convert numpy arrays to lists)\n",
    "    save_data = {\n",
    "        'accuracy': float(evaluation_results['accuracy']),\n",
    "        'top_3_accuracy': float(evaluation_results['top_3_accuracy']),\n",
    "        'classification_report': evaluation_results['classification_report'],\n",
    "        'confusion_matrix': evaluation_results['confusion_matrix'].tolist(),\n",
    "        'per_class_accuracy': evaluation_results['per_class_accuracy'].tolist(),\n",
    "        'class_names': evaluation_results['class_names'].tolist(),\n",
    "        'evaluation_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(save_data, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Evaluation results saved to: {save_path}\")\n",
    "\n",
    "# Evaluate the model\n",
    "if all(var in locals() for var in ['best_model', 'test_dataset', 'label_encoder', 'experiment_dir']):\n",
    "    evaluation_results = evaluate_model(best_model, test_dataset, label_encoder)\n",
    "    \n",
    "    # Analyze misclassifications\n",
    "    misclassifications = analyze_misclassifications(evaluation_results)\n",
    "    \n",
    "    # Save results\n",
    "    eval_results_path = os.path.join(experiment_dir, \"evaluation_results.json\")\n",
    "    save_evaluation_results(evaluation_results, eval_results_path)\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Model evaluation completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Missing required components for evaluation.\")\n",
    "    missing = []\n",
    "    if 'best_model' not in locals():\n",
    "        missing.append('best_model')\n",
    "    if 'test_dataset' not in locals():\n",
    "        missing.append('test_dataset')\n",
    "    if 'label_encoder' not in locals():\n",
    "        missing.append('label_encoder')\n",
    "    if 'experiment_dir' not in locals():\n",
    "        missing.append('experiment_dir')\n",
    "    print(f\"Missing: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22b1c1",
   "metadata": {},
   "source": [
    "## 12. Performance Visualization\n",
    "\n",
    "Creating comprehensive visualizations for training history and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training history with loss and accuracy curves.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training and validation loss\n",
    "    axes[0, 0].plot(history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training and validation accuracy\n",
    "    axes[0, 1].plot(history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate (if available)\n",
    "    if 'lr' in history:\n",
    "        axes[1, 0].plot(history['lr'], label='Learning Rate', color='green', linewidth=2)\n",
    "        axes[1, 0].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'Learning Rate\\\\nNot Available', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes,\n",
    "                       fontsize=12)\n",
    "        axes[1, 0].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Top-3 accuracy (if available)\n",
    "    if 'top_3_accuracy' in history and 'val_top_3_accuracy' in history:\n",
    "        axes[1, 1].plot(history['top_3_accuracy'], label='Training Top-3', color='blue', linewidth=2)\n",
    "        axes[1, 1].plot(history['val_top_3_accuracy'], label='Validation Top-3', color='red', linewidth=2)\n",
    "        axes[1, 1].set_title('Top-3 Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Top-3 Accuracy')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Show final metrics\n",
    "        final_train_acc = history['accuracy'][-1]\n",
    "        final_val_acc = history['val_accuracy'][-1]\n",
    "        axes[1, 1].bar(['Train', 'Validation'], [final_train_acc, final_val_acc], \n",
    "                      color=['blue', 'red'], alpha=0.7)\n",
    "        axes[1, 1].set_title('Final Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Accuracy')\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        for i, v in enumerate([final_train_acc, final_val_acc]):\n",
    "            axes[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üìä Training history plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm_normalized, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Normalized Count'})\n",
    "    \n",
    "    plt.title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üìä Confusion matrix plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_per_class_metrics(evaluation_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot per-class performance metrics.\n",
    "    \"\"\"\n",
    "    class_names = evaluation_results['class_names']\n",
    "    report = evaluation_results['classification_report']\n",
    "    \n",
    "    # Extract metrics for each class\n",
    "    metrics = ['precision', 'recall', 'f1-score']\n",
    "    metric_values = {metric: [] for metric in metrics}\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        for metric in metrics:\n",
    "            metric_values[metric].append(report[class_name][metric])\n",
    "    \n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    \n",
    "    for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "        axes[i].bar(x, metric_values[metric], color=color, alpha=0.8, width=0.6)\n",
    "        axes[i].set_title(f'{metric.capitalize()} per Class', fontsize=14, fontweight='bold')\n",
    "        axes[i].set_xlabel('Raga Class')\n",
    "        axes[i].set_ylabel(metric.capitalize())\n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for j, v in enumerate(metric_values[metric]):\n",
    "            axes[i].text(j, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üìä Per-class metrics plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def create_interactive_plots(evaluation_results, history):\n",
    "    \"\"\"\n",
    "    Create interactive plots using Plotly.\n",
    "    \"\"\"\n",
    "    # Training history\n",
    "    fig_history = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Loss', 'Accuracy', 'Learning Rate', 'Final Metrics'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    epochs = list(range(1, len(history['loss']) + 1))\n",
    "    \n",
    "    # Loss plot\n",
    "    fig_history.add_trace(\n",
    "        go.Scatter(x=epochs, y=history['loss'], name='Training Loss', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig_history.add_trace(\n",
    "        go.Scatter(x=epochs, y=history['val_loss'], name='Validation Loss', line=dict(color='red')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Accuracy plot\n",
    "    fig_history.add_trace(\n",
    "        go.Scatter(x=epochs, y=history['accuracy'], name='Training Accuracy', line=dict(color='blue')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig_history.add_trace(\n",
    "        go.Scatter(x=epochs, y=history['val_accuracy'], name='Validation Accuracy', line=dict(color='red')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Learning rate (if available)\n",
    "    if 'lr' in history:\n",
    "        fig_history.add_trace(\n",
    "            go.Scatter(x=epochs, y=history['lr'], name='Learning Rate', line=dict(color='green')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Final metrics bar chart\n",
    "    final_metrics = ['Train Acc', 'Val Acc', 'Test Acc']\n",
    "    final_values = [\n",
    "        history['accuracy'][-1], \n",
    "        history['val_accuracy'][-1],\n",
    "        evaluation_results['accuracy']\n",
    "    ]\n",
    "    \n",
    "    fig_history.add_trace(\n",
    "        go.Bar(x=final_metrics, y=final_values, name='Final Metrics'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig_history.update_layout(height=800, title_text=\"Training History Dashboard\")\n",
    "    fig_history.show()\n",
    "    \n",
    "    # Confusion matrix heatmap\n",
    "    cm = evaluation_results['confusion_matrix']\n",
    "    class_names = evaluation_results['class_names']\n",
    "    \n",
    "    fig_cm = go.Figure(data=go.Heatmap(\n",
    "        z=cm,\n",
    "        x=class_names,\n",
    "        y=class_names,\n",
    "        colorscale='Blues',\n",
    "        text=cm,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 12},\n",
    "    ))\n",
    "    \n",
    "    fig_cm.update_layout(\n",
    "        title=\"Confusion Matrix\",\n",
    "        xaxis_title=\"Predicted Label\",\n",
    "        yaxis_title=\"True Label\",\n",
    "        width=600,\n",
    "        height=600\n",
    "    )\n",
    "    fig_cm.show()\n",
    "\n",
    "# Create visualizations\n",
    "if all(var in locals() for var in ['history', 'evaluation_results', 'experiment_dir']):\n",
    "    print(\"üìä Creating performance visualizations...\")\n",
    "    \n",
    "    # Training history plot\n",
    "    history_plot_path = os.path.join(experiment_dir, \"training_history.png\")\n",
    "    plot_training_history(history.history, history_plot_path)\n",
    "    \n",
    "    # Confusion matrix plot\n",
    "    cm_plot_path = os.path.join(experiment_dir, \"confusion_matrix.png\")\n",
    "    plot_confusion_matrix(\n",
    "        evaluation_results['confusion_matrix'], \n",
    "        evaluation_results['class_names'], \n",
    "        cm_plot_path\n",
    "    )\n",
    "    \n",
    "    # Per-class metrics plot\n",
    "    metrics_plot_path = os.path.join(experiment_dir, \"per_class_metrics.png\")\n",
    "    plot_per_class_metrics(evaluation_results, metrics_plot_path)\n",
    "    \n",
    "    # Interactive plots\n",
    "    print(\"\\\\nüìä Creating interactive plots...\")\n",
    "    create_interactive_plots(evaluation_results, history.history)\n",
    "    \n",
    "    print(\"‚úÖ All visualizations created successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Missing data for visualization.\")\n",
    "    missing = []\n",
    "    if 'history' not in locals():\n",
    "        missing.append('history')\n",
    "    if 'evaluation_results' not in locals():\n",
    "        missing.append('evaluation_results')\n",
    "    if 'experiment_dir' not in locals():\n",
    "        missing.append('experiment_dir')\n",
    "    print(f\"Missing: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20844c",
   "metadata": {},
   "source": [
    "## 13. Model Export and Saving\n",
    "\n",
    "Saving the trained model in TensorFlow SavedModel format and creating deployment files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd035983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_deployment(model, save_dir, label_encoder, model_config):\n",
    "    \"\"\"\n",
    "    Save model and all necessary files for deployment.\n",
    "    \"\"\"\n",
    "    print(f\"üíæ Saving model for deployment to: {save_dir}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the model in SavedModel format\n",
    "    model_path = os.path.join(save_dir, \"saved_model\")\n",
    "    model.save(model_path, save_format='tf')\n",
    "    print(f\"‚úÖ Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save label encoder\n",
    "    label_encoder_path = os.path.join(save_dir, \"label_encoder.pkl\")\n",
    "    with open(label_encoder_path, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(f\"‚úÖ Label encoder saved to: {label_encoder_path}\")\n",
    "    \n",
    "    # Save model configuration\n",
    "    config_path = os.path.join(save_dir, \"config.json\")\n",
    "    full_config = {\n",
    "        **model_config,\n",
    "        'class_names': label_encoder.classes_.tolist(),\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'yamnet_sample_rate': YAMNET_SAMPLE_RATE,\n",
    "        'yamnet_duration': YAMNET_DURATION,\n",
    "        'yamnet_samples': YAMNET_SAMPLES,\n",
    "        'model_version': '1.0.0',\n",
    "        'creation_date': datetime.now().isoformat(),\n",
    "        'framework': 'tensorflow',\n",
    "        'base_model': 'yamnet'\n",
    "    }\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(full_config, f, indent=2)\n",
    "    print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
    "    \n",
    "    # Create inference script\n",
    "    inference_script = f'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import librosa\n",
    "\n",
    "class RagaClassifier:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = tf.saved_model.load(model_path)\n",
    "        \n",
    "        # Load configuration\n",
    "        with open(f\"{{model_path}}/../config.json\", 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Load label encoder\n",
    "        with open(f\"{{model_path}}/../label_encoder.pkl\", 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)\n",
    "        \n",
    "        self.sample_rate = self.config['yamnet_sample_rate']\n",
    "        self.duration = self.config['yamnet_duration']\n",
    "        self.samples = self.config['yamnet_samples']\n",
    "        \n",
    "    def preprocess_audio(self, audio_path):\n",
    "        \"\"\"Preprocess audio file for prediction.\"\"\"\n",
    "        # Load audio\n",
    "        audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        \n",
    "        # Normalize\n",
    "        audio = librosa.util.normalize(audio)\n",
    "        \n",
    "        # Ensure correct length\n",
    "        if len(audio) < self.samples:\n",
    "            audio = np.pad(audio, (0, self.samples - len(audio)), mode='constant')\n",
    "        else:\n",
    "            audio = audio[:self.samples]\n",
    "        \n",
    "        return audio.astype(np.float32)\n",
    "    \n",
    "    def predict(self, audio_path):\n",
    "        \"\"\"Predict raga from audio file.\"\"\"\n",
    "        # Preprocess audio\n",
    "        audio = self.preprocess_audio(audio_path)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        audio_batch = np.expand_dims(audio, axis=0)\n",
    "        \n",
    "        # Get prediction\n",
    "        predictions = self.model(audio_batch)\n",
    "        probabilities = tf.nn.softmax(predictions).numpy()[0]\n",
    "        \n",
    "        # Get class predictions\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        predicted_raga = self.label_encoder.inverse_transform([predicted_class])[0]\n",
    "        confidence = probabilities[predicted_class]\n",
    "        \n",
    "        # Get top-3 predictions\n",
    "        top_3_indices = np.argsort(probabilities)[-3:][::-1]\n",
    "        top_3_predictions = []\n",
    "        \n",
    "        for idx in top_3_indices:\n",
    "            raga = self.label_encoder.inverse_transform([idx])[0]\n",
    "            prob = probabilities[idx]\n",
    "            top_3_predictions.append({{'raga': raga, 'confidence': float(prob)}})\n",
    "        \n",
    "        return {{\n",
    "            'predicted_raga': predicted_raga,\n",
    "            'confidence': float(confidence),\n",
    "            'top_3_predictions': top_3_predictions,\n",
    "            'all_probabilities': {{\n",
    "                self.label_encoder.inverse_transform([i])[0]: float(prob) \n",
    "                for i, prob in enumerate(probabilities)\n",
    "            }}\n",
    "        }}\n",
    "\n",
    "# Example usage:\n",
    "# classifier = RagaClassifier(\"path/to/saved_model\")\n",
    "# result = classifier.predict(\"path/to/audio.wav\")\n",
    "# print(f\"Predicted raga: {{result['predicted_raga']}} ({{result['confidence']:.3f}})\")\n",
    "'''\n",
    "    \n",
    "    inference_script_path = os.path.join(save_dir, \"inference.py\")\n",
    "    with open(inference_script_path, 'w') as f:\n",
    "        f.write(inference_script)\n",
    "    print(f\"‚úÖ Inference script saved to: {inference_script_path}\")\n",
    "    \n",
    "    # Create requirements.txt\n",
    "    requirements = '''tensorflow>=2.8.0\n",
    "numpy>=1.21.0\n",
    "librosa>=0.9.0\n",
    "scikit-learn>=1.0.0\n",
    "'''\n",
    "    \n",
    "    requirements_path = os.path.join(save_dir, \"requirements.txt\")\n",
    "    with open(requirements_path, 'w') as f:\n",
    "        f.write(requirements)\n",
    "    print(f\"‚úÖ Requirements file saved to: {requirements_path}\")\n",
    "    \n",
    "    # Create README\n",
    "    readme_content = f'''# YAMNet Raga Classifier\n",
    "\n",
    "This model classifies Indian classical music ragas using a fine-tuned YAMNet architecture.\n",
    "\n",
    "## Model Information\n",
    "- Base Model: YAMNet (Google)\n",
    "- Classes: {len(label_encoder.classes_)} ragas\n",
    "- Input: 16kHz audio, {YAMNET_DURATION}s duration\n",
    "- Framework: TensorFlow {tf.__version__}\n",
    "\n",
    "## Classes\n",
    "{chr(10).join([f\"- {cls}\" for cls in label_encoder.classes_])}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from inference import RagaClassifier\n",
    "\n",
    "# Load the classifier\n",
    "classifier = RagaClassifier(\"saved_model\")\n",
    "\n",
    "# Predict raga\n",
    "result = classifier.predict(\"audio_file.wav\")\n",
    "print(f\"Predicted: {{result['predicted_raga']}} ({{result['confidence']:.3f}})\")\n",
    "```\n",
    "\n",
    "## Files\n",
    "- `saved_model/`: TensorFlow SavedModel\n",
    "- `config.json`: Model configuration\n",
    "- `label_encoder.pkl`: Label encoder for class mapping\n",
    "- `inference.py`: Inference script\n",
    "- `requirements.txt`: Python dependencies\n",
    "\n",
    "## Training Details\n",
    "- Training Date: {datetime.now().strftime(\"%Y-%m-%d\")}\n",
    "- Model Version: 1.0.0\n",
    "- Base Architecture: YAMNet + Custom Classification Head\n",
    "'''\n",
    "    \n",
    "    readme_path = os.path.join(save_dir, \"README.md\")\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    print(f\"‚úÖ README saved to: {readme_path}\")\n",
    "    \n",
    "    return save_dir\n",
    "\n",
    "def test_saved_model(save_dir, test_audio_path=None):\n",
    "    \"\"\"\n",
    "    Test the saved model to ensure it works correctly.\n",
    "    \"\"\"\n",
    "    print(f\"üß™ Testing saved model...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the saved model\n",
    "        model_path = os.path.join(save_dir, \"saved_model\")\n",
    "        loaded_model = tf.saved_model.load(model_path)\n",
    "        \n",
    "        # Test with dummy input\n",
    "        dummy_input = tf.random.normal([1, YAMNET_SAMPLES])\n",
    "        output = loaded_model(dummy_input)\n",
    "        \n",
    "        print(f\"‚úÖ Model loading test passed!\")\n",
    "        print(f\"   Input shape: {dummy_input.shape}\")\n",
    "        print(f\"   Output shape: {output.shape}\")\n",
    "        \n",
    "        # Test inference script if test audio is provided\n",
    "        if test_audio_path and os.path.exists(test_audio_path):\n",
    "            print(f\"üß™ Testing inference script with: {test_audio_path}\")\n",
    "            \n",
    "            # This would require the inference script to be imported\n",
    "            # For now, just verify the files exist\n",
    "            required_files = [\"config.json\", \"label_encoder.pkl\", \"inference.py\", \"README.md\"]\n",
    "            for file in required_files:\n",
    "                file_path = os.path.join(save_dir, file)\n",
    "                if os.path.exists(file_path):\n",
    "                    print(f\"   ‚úÖ {file} exists\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå {file} missing\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model testing failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Save the model for deployment\n",
    "if all(var in locals() for var in ['best_model', 'label_encoder', 'model_config', 'experiment_dir']):\n",
    "    deployment_dir = os.path.join(experiment_dir, \"deployment\")\n",
    "    \n",
    "    saved_model_dir = save_model_for_deployment(\n",
    "        best_model, \n",
    "        deployment_dir, \n",
    "        label_encoder, \n",
    "        model_config\n",
    "    )\n",
    "    \n",
    "    # Test the saved model\n",
    "    test_success = test_saved_model(deployment_dir)\n",
    "    \n",
    "    if test_success:\n",
    "        print(f\"\\\\nüéâ Model successfully saved for deployment!\")\n",
    "        print(f\"üìÅ Deployment directory: {deployment_dir}\")\n",
    "        print(f\"üì¶ Model size: {sum(os.path.getsize(os.path.join(dirpath, filename)) for dirpath, dirnames, filenames in os.walk(deployment_dir) for filename in filenames) / (1024*1024):.1f} MB\")\n",
    "        \n",
    "        # Create a zip file for easy sharing\n",
    "        import shutil\n",
    "        zip_path = os.path.join(experiment_dir, \"raga_classifier_model\")\n",
    "        shutil.make_archive(zip_path, 'zip', deployment_dir)\n",
    "        print(f\"üì¶ Model package created: {zip_path}.zip\")\n",
    "    else:\n",
    "        print(f\"‚ùå Model saving failed validation\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Missing required components for model saving.\")\n",
    "    missing = []\n",
    "    if 'best_model' not in locals():\n",
    "        missing.append('best_model')\n",
    "    if 'label_encoder' not in locals():\n",
    "        missing.append('label_encoder')\n",
    "    if 'model_config' not in locals():\n",
    "        missing.append('model_config')\n",
    "    if 'experiment_dir' not in locals():\n",
    "        missing.append('experiment_dir')\n",
    "    print(f\"Missing: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2f888",
   "metadata": {},
   "source": [
    "## 14. HuggingFace Hub Integration\n",
    "\n",
    "Uploading the trained model to HuggingFace Hub with proper documentation and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab57219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Hub configuration\n",
    "HF_USERNAME = \"your-username\"  # Replace with your HuggingFace username\n",
    "HF_MODEL_NAME = \"yamnet-raga-classifier\"\n",
    "HF_TOKEN = None  # Will be set during login\n",
    "\n",
    "def create_model_card(evaluation_results, model_config, training_config):\n",
    "    \"\"\"\n",
    "    Create a comprehensive model card for HuggingFace Hub.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_card = f'''---\n",
    "language: en\n",
    "tags:\n",
    "- audio-classification\n",
    "- music\n",
    "- indian-classical\n",
    "- raga\n",
    "- yamnet\n",
    "- tensorflow\n",
    "license: mit\n",
    "datasets:\n",
    "- custom\n",
    "metrics:\n",
    "- accuracy\n",
    "- f1\n",
    "model-index:\n",
    "- name: {HF_MODEL_NAME}\n",
    "  results:\n",
    "  - task:\n",
    "      type: audio-classification\n",
    "      name: Audio Classification\n",
    "    dataset:\n",
    "      type: custom\n",
    "      name: Indian Classical Raga Dataset\n",
    "    metrics:\n",
    "    - type: accuracy\n",
    "      value: {evaluation_results['accuracy']:.4f}\n",
    "    - type: f1\n",
    "      value: {evaluation_results['classification_report']['macro avg']['f1-score']:.4f}\n",
    "---\n",
    "\n",
    "# YAMNet Fine-tuned for Indian Classical Raga Classification\n",
    "\n",
    "This model is a fine-tuned version of [YAMNet](https://tfhub.dev/google/yamnet/1) for classifying Indian classical music ragas.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This model uses Google's YAMNet as a feature extractor and adds a custom classification head for identifying {model_config['num_classes']} different Indian classical ragas.\n",
    "\n",
    "### Model Architecture\n",
    "- **Base Model**: YAMNet (Google)\n",
    "- **Input**: 16kHz audio, {YAMNET_DURATION}s duration ({YAMNET_SAMPLES} samples)\n",
    "- **Output**: Softmax probabilities over {model_config['num_classes']} raga classes\n",
    "- **Framework**: TensorFlow {tf.__version__}\n",
    "\n",
    "### Supported Ragas\n",
    "{chr(10).join([f\"- {cls}\" for cls in evaluation_results['class_names']])}\n",
    "\n",
    "## Training Details\n",
    "\n",
    "### Training Data\n",
    "- **Dataset**: Custom Indian Classical Raga Dataset\n",
    "- **Classes**: {model_config['num_classes']} ragas\n",
    "- **Audio Format**: 16kHz WAV files\n",
    "- **Augmentation**: Time stretching, pitch shifting, noise addition\n",
    "\n",
    "### Training Configuration\n",
    "- **Optimizer**: Adam\n",
    "- **Learning Rate**: {training_config['initial_learning_rate']}\n",
    "- **Batch Size**: Variable (adaptive based on audio segments)\n",
    "- **Epochs**: {training_config['epochs']}\n",
    "- **Early Stopping**: Patience of {training_config['patience']} epochs\n",
    "\n",
    "### Training Results\n",
    "- **Best Validation Accuracy**: {max([v for v in evaluation_results.get('val_accuracies', [evaluation_results['accuracy']])]):.4f}\n",
    "- **Test Accuracy**: {evaluation_results['accuracy']:.4f}\n",
    "- **Test F1-Score (Macro)**: {evaluation_results['classification_report']['macro avg']['f1-score']:.4f}\n",
    "- **Test F1-Score (Weighted)**: {evaluation_results['classification_report']['weighted avg']['f1-score']:.4f}\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Using the Model\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Load the model\n",
    "model = tf.saved_model.load(\"path/to/model\")\n",
    "\n",
    "# Preprocess audio\n",
    "def preprocess_audio(audio_path):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    \n",
    "    # Ensure correct length\n",
    "    target_samples = {YAMNET_SAMPLES}\n",
    "    if len(audio) < target_samples:\n",
    "        audio = np.pad(audio, (0, target_samples - len(audio)), mode='constant')\n",
    "    else:\n",
    "        audio = audio[:target_samples]\n",
    "    \n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "# Make prediction\n",
    "audio = preprocess_audio(\"your_audio.wav\")\n",
    "audio_batch = np.expand_dims(audio, axis=0)\n",
    "predictions = model(audio_batch)\n",
    "probabilities = tf.nn.softmax(predictions).numpy()[0]\n",
    "\n",
    "# Get predicted class\n",
    "predicted_class = np.argmax(probabilities)\n",
    "confidence = probabilities[predicted_class]\n",
    "\n",
    "print(f\"Predicted raga index: {{predicted_class}} (confidence: {{confidence:.3f}})\")\n",
    "```\n",
    "\n",
    "### Class Mapping\n",
    "The model outputs integer class indices. Here's the mapping to raga names:\n",
    "\n",
    "```python\n",
    "class_names = {evaluation_results['class_names']}\n",
    "predicted_raga = class_names[predicted_class]\n",
    "```\n",
    "\n",
    "## Performance\n",
    "\n",
    "### Overall Metrics\n",
    "- **Accuracy**: {evaluation_results['accuracy']:.4f}\n",
    "- **Macro F1-Score**: {evaluation_results['classification_report']['macro avg']['f1-score']:.4f}\n",
    "- **Weighted F1-Score**: {evaluation_results['classification_report']['weighted avg']['f1-score']:.4f}\n",
    "\n",
    "### Per-Class Performance\n",
    "| Raga | Precision | Recall | F1-Score |\n",
    "|------|-----------|--------|----------|'''\n",
    "\n",
    "    # Add per-class metrics to the table\n",
    "    for class_name in evaluation_results['class_names']:\n",
    "        metrics = evaluation_results['classification_report'][class_name]\n",
    "        model_card += f'''\n",
    "| {class_name} | {metrics['precision']:.3f} | {metrics['recall']:.3f} | {metrics['f1-score']:.3f} |'''\n",
    "\n",
    "    model_card += f'''\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- The model is trained on a specific dataset and may not generalize to all styles of Indian classical music\n",
    "- Performance may vary with different recording qualities, instruments, or vocal styles\n",
    "- The model requires audio to be exactly {YAMNET_DURATION}s long and sampled at 16kHz\n",
    "\n",
    "## Bias and Fairness\n",
    "\n",
    "This model may exhibit bias based on:\n",
    "- The composition of the training dataset\n",
    "- Recording quality and conditions\n",
    "- Specific musical instruments or vocal styles present in training data\n",
    "- Regional variations in raga interpretation\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this model, please cite:\n",
    "\n",
    "```bibtex\n",
    "@misc{{yamnet-raga-classifier,\n",
    "  title={{YAMNet Fine-tuned for Indian Classical Raga Classification}},\n",
    "  author={{Your Name}},\n",
    "  year={{2024}},\n",
    "  url={{https://huggingface.co/{HF_USERNAME}/{HF_MODEL_NAME}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "This model is released under the MIT License.\n",
    "\n",
    "## Contact\n",
    "\n",
    "For questions or issues, please contact [your-email@example.com](mailto:your-email@example.com).\n",
    "'''\n",
    "\n",
    "    return model_card\n",
    "\n",
    "def upload_to_huggingface(model_dir, model_card_content, evaluation_results):\n",
    "    \"\"\"\n",
    "    Upload model to HuggingFace Hub with proper documentation.\n",
    "    \"\"\"\n",
    "    global HF_TOKEN\n",
    "    \n",
    "    try:\n",
    "        # Login to HuggingFace (this will prompt for token if not provided)\n",
    "        if HF_TOKEN is None:\n",
    "            from huggingface_hub import notebook_login\n",
    "            notebook_login()\n",
    "            # Get token after login\n",
    "            from huggingface_hub import HfFolder\n",
    "            HF_TOKEN = HfFolder.get_token()\n",
    "        \n",
    "        # Initialize HF API\n",
    "        api = HfApi(token=HF_TOKEN)\n",
    "        \n",
    "        # Create repository\n",
    "        repo_id = f\"{HF_USERNAME}/{HF_MODEL_NAME}\"\n",
    "        \n",
    "        try:\n",
    "            create_repo(repo_id, token=HF_TOKEN, exist_ok=True)\n",
    "            print(f\"‚úÖ Repository created/verified: {repo_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Repository creation warning: {e}\")\n",
    "        \n",
    "        # Upload model files\n",
    "        model_files = []\n",
    "        \n",
    "        # Find all files in the model directory\n",
    "        for root, dirs, files in os.walk(model_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(file_path, model_dir)\n",
    "                model_files.append((file_path, relative_path))\n",
    "        \n",
    "        print(f\"üì§ Uploading {len(model_files)} files...\")\n",
    "        \n",
    "        # Upload each file\n",
    "        for file_path, relative_path in tqdm(model_files, desc=\"Uploading files\"):\n",
    "            try:\n",
    "                upload_file(\n",
    "                    path_or_fileobj=file_path,\n",
    "                    path_in_repo=relative_path,\n",
    "                    repo_id=repo_id,\n",
    "                    token=HF_TOKEN\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Failed to upload {relative_path}: {e}\")\n",
    "        \n",
    "        # Create and upload model card\n",
    "        model_card_path = os.path.join(model_dir, \"README.md\")\n",
    "        with open(model_card_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(model_card_content)\n",
    "        \n",
    "        upload_file(\n",
    "            path_or_fileobj=model_card_path,\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=repo_id,\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model uploaded successfully!\")\n",
    "        print(f\"üîó Model URL: https://huggingface.co/{repo_id}\")\n",
    "        \n",
    "        return repo_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def create_usage_example(repo_id, class_names):\n",
    "    \"\"\"\n",
    "    Create a usage example notebook for the uploaded model.\n",
    "    \"\"\"\n",
    "    \n",
    "    example_code = f'''# YAMNet Raga Classifier - Usage Example\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install tensorflow huggingface_hub librosa numpy\n",
    "```\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download model from HuggingFace Hub\n",
    "model_path = snapshot_download(repo_id=\"{repo_id}\")\n",
    "\n",
    "# Load the model\n",
    "model = tf.saved_model.load(model_path + \"/saved_model\")\n",
    "\n",
    "# Class names\n",
    "class_names = {class_names}\n",
    "\n",
    "def predict_raga(audio_path):\n",
    "    # Load and preprocess audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    \n",
    "    # Ensure correct length\n",
    "    target_samples = {YAMNET_SAMPLES}\n",
    "    if len(audio) < target_samples:\n",
    "        audio = np.pad(audio, (0, target_samples - len(audio)), mode='constant')\n",
    "    else:\n",
    "        audio = audio[:target_samples]\n",
    "    \n",
    "    # Add batch dimension\n",
    "    audio_batch = np.expand_dims(audio.astype(np.float32), axis=0)\n",
    "    \n",
    "    # Get prediction\n",
    "    predictions = model(audio_batch)\n",
    "    probabilities = tf.nn.softmax(predictions).numpy()[0]\n",
    "    \n",
    "    # Get top predictions\n",
    "    top_indices = np.argsort(probabilities)[-3:][::-1]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({{\n",
    "            'raga': class_names[idx],\n",
    "            'confidence': float(probabilities[idx])\n",
    "        }})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "# results = predict_raga(\"path/to/your/audio.wav\")\n",
    "# print(f\"Top prediction: {{results[0]['raga']}} ({{results[0]['confidence']:.3f}})\")\n",
    "```\n",
    "\n",
    "## Batch Processing\n",
    "\n",
    "```python\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_audio_folder(folder_path):\n",
    "    results = {{}}\n",
    "    \n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith(('.wav', '.mp3', '.flac')):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                predictions = predict_raga(file_path)\n",
    "                results[filename] = predictions[0]  # Top prediction\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {{filename}}: {{e}}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process a folder of audio files\n",
    "# results = process_audio_folder(\"path/to/audio/folder\")\n",
    "```\n",
    "'''\n",
    "    \n",
    "    return example_code\n",
    "\n",
    "# Upload to HuggingFace Hub\n",
    "if all(var in locals() for var in ['evaluation_results', 'model_config', 'TRAINING_CONFIG', 'deployment_dir']):\n",
    "    print(\"ü§ó Preparing to upload to HuggingFace Hub...\")\n",
    "    \n",
    "    # Update HuggingFace configuration\n",
    "    print(\"üìù Please update the HuggingFace configuration:\")\n",
    "    print(f\"   HF_USERNAME: {HF_USERNAME}\")\n",
    "    print(f\"   HF_MODEL_NAME: {HF_MODEL_NAME}\")\n",
    "    print(\"\\\\nIf these are correct, proceed. Otherwise, update the variables above.\")\n",
    "    \n",
    "    # Create model card\n",
    "    print(\"üìÑ Creating model card...\")\n",
    "    model_card = create_model_card(evaluation_results, model_config, TRAINING_CONFIG)\n",
    "    \n",
    "    # Save model card locally\n",
    "    model_card_path = os.path.join(deployment_dir, \"MODEL_CARD.md\")\n",
    "    with open(model_card_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(model_card)\n",
    "    print(f\"üíæ Model card saved locally: {model_card_path}\")\n",
    "    \n",
    "    # Create usage example\n",
    "    usage_example = create_usage_example(\n",
    "        f\"{HF_USERNAME}/{HF_MODEL_NAME}\", \n",
    "        evaluation_results['class_names'].tolist()\n",
    "    )\n",
    "    \n",
    "    usage_example_path = os.path.join(deployment_dir, \"USAGE_EXAMPLE.md\")\n",
    "    with open(usage_example_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(usage_example)\n",
    "    print(f\"üíæ Usage example saved: {usage_example_path}\")\n",
    "    \n",
    "    # Ask for confirmation before uploading\n",
    "    print(\"\\\\nüöÄ Ready to upload to HuggingFace Hub!\")\n",
    "    print(\"‚ö†Ô∏è  This will make your model publicly available.\")\n",
    "    print(\"\\\\nTo proceed with upload, uncomment and run the following line:\")\n",
    "    print(\"# repo_id = upload_to_huggingface(deployment_dir, model_card, evaluation_results)\")\n",
    "    \n",
    "    # Uncomment the line below to actually upload\n",
    "    # repo_id = upload_to_huggingface(deployment_dir, model_card, evaluation_results)\n",
    "    \n",
    "    print(\"\\\\nüìã Summary of what's ready for upload:\")\n",
    "    print(f\"   Model files: {len([f for f in os.listdir(deployment_dir) if os.path.isfile(os.path.join(deployment_dir, f))])} files\")\n",
    "    print(f\"   Model card: ‚úÖ\")\n",
    "    print(f\"   Usage example: ‚úÖ\")\n",
    "    print(f\"   Configuration: ‚úÖ\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Missing required components for HuggingFace upload.\")\n",
    "    missing = []\n",
    "    if 'evaluation_results' not in locals():\n",
    "        missing.append('evaluation_results')\n",
    "    if 'model_config' not in locals():\n",
    "        missing.append('model_config')\n",
    "    if 'TRAINING_CONFIG' not in locals():\n",
    "        missing.append('TRAINING_CONFIG')\n",
    "    if 'deployment_dir' not in locals():\n",
    "        missing.append('deployment_dir')\n",
    "    print(f\"Missing: {missing}\")\n",
    "\n",
    "print(\"\\\\nüéâ Notebook execution complete!\")\n",
    "print(\"\\\\nüìä Final Summary:\")\n",
    "print(\"‚úÖ Environment setup and GPU configuration\")\n",
    "print(\"‚úÖ Library installation and imports\")\n",
    "print(\"‚úÖ Google Drive integration and dataset loading\")\n",
    "print(\"‚úÖ Audio preprocessing pipeline\")\n",
    "print(\"‚úÖ Dataset preparation and splitting\")\n",
    "print(\"‚úÖ YAMNet model loading and feature extraction\")\n",
    "print(\"‚úÖ Custom classification head architecture\")\n",
    "print(\"‚úÖ Data augmentation and generators\")\n",
    "print(\"‚úÖ Training configuration and callbacks\")\n",
    "print(\"‚úÖ Model training loop\")\n",
    "print(\"‚úÖ Model evaluation and metrics\")\n",
    "print(\"‚úÖ Performance visualization\")\n",
    "print(\"‚úÖ Model export and saving\")\n",
    "print(\"‚úÖ HuggingFace Hub integration preparation\")\n",
    "\n",
    "print(\"\\\\nüöÄ Your YAMNet raga classifier is ready for deployment!\")\n",
    "print(\"üìÅ Check the experiment directory for all outputs and saved models.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
