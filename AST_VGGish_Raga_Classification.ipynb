{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab11a736",
   "metadata": {},
   "source": [
    "# Audio Spectrogram Transformer (AST) vs VGGish for Raga Classification\n",
    "\n",
    "This comprehensive notebook compares two state-of-the-art audio classification models for Indian classical raga identification:\n",
    "\n",
    "## Models Compared\n",
    "\n",
    "### üéµ **Audio Spectrogram Transformer (AST)**\n",
    "- **Architecture**: Vision Transformer adapted for audio spectrograms\n",
    "- **Input**: Mel spectrograms (128 mel bins √ó 1024 time frames)\n",
    "- **Framework**: PyTorch + Hugging Face Transformers\n",
    "- **Strengths**: Self-attention mechanisms, long-range dependencies\n",
    "\n",
    "### üéµ **VGGish**\n",
    "- **Architecture**: CNN-based feature extractor (VGG-like)\n",
    "- **Input**: Audio waveform ‚Üí VGGish embeddings (128-dim)\n",
    "- **Framework**: TensorFlow + TensorFlow Hub\n",
    "- **Strengths**: Proven audio features, computational efficiency\n",
    "\n",
    "## Notebook Objectives\n",
    "\n",
    "1. üîß **Fair Comparison**: Same dataset, evaluation metrics, and preprocessing\n",
    "2. üìä **Comprehensive Analysis**: Training curves, confusion matrices, performance metrics\n",
    "3. üöÄ **Production Ready**: Model saving, documentation, HuggingFace Hub deployment\n",
    "4. üìù **Reproducible**: Clear code structure with proper error handling\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- GPU-enabled environment (Google Colab Pro recommended)\n",
    "- Python 3.8+\n",
    "- PyTorch with CUDA support\n",
    "- TensorFlow 2.x\n",
    "- Hugging Face transformers library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea1554",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "Setting up the environment for both PyTorch (AST) and TensorFlow (VGGish) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers datasets accelerate\n",
    "!pip install tensorflow tensorflow-hub\n",
    "!pip install librosa soundfile audiomentations\n",
    "!pip install scikit-learn matplotlib seaborn plotly\n",
    "!pip install huggingface_hub wandb tensorboard\n",
    "!pip install ipywidgets tqdm pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab432687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import logging\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# HuggingFace transformers\n",
    "from transformers import (\n",
    "    ASTForAudioClassification, ASTFeatureExtractor,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback,\n",
    "    AutoConfig, AutoModel, AutoFeatureExtractor\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "# TensorFlow imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "\n",
    "# Data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Utilities\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import HfApi, create_repo, upload_file\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üì¶ All packages imported successfully!\")\n",
    "\n",
    "# GPU Configuration\n",
    "def setup_device_config():\n",
    "    \"\"\"Configure PyTorch and TensorFlow for optimal GPU usage.\"\"\"\n",
    "    \n",
    "    # PyTorch CUDA setup\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"üöÄ PyTorch CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "        # Set memory fraction\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"‚ö†Ô∏è  PyTorch: No CUDA available, using CPU\")\n",
    "    \n",
    "    # TensorFlow GPU setup\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"‚úÖ TensorFlow: Found {len(gpus)} GPU(s), memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"‚ùå TensorFlow GPU setup error: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  TensorFlow: No GPU found, using CPU\")\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    print(\"üé≤ Random seeds set for reproducibility\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Setup device\n",
    "DEVICE = setup_device_config()\n",
    "\n",
    "# Global configuration\n",
    "CONFIG = {\n",
    "    'batch_size': 16,\n",
    "    'learning_rate_ast': 5e-5,\n",
    "    'learning_rate_vggish': 1e-3,\n",
    "    'epochs': 50,\n",
    "    'patience': 10,\n",
    "    'warmup_steps': 500,\n",
    "    'weight_decay': 0.01,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'label_smoothing': 0.1,\n",
    "    \n",
    "    # Audio configuration\n",
    "    'sample_rate': 16000,\n",
    "    'ast_max_length': 1024,  # AST expects 1024 time frames\n",
    "    'ast_num_mel_bins': 128,\n",
    "    'vggish_embedding_dim': 128,\n",
    "    \n",
    "    # Augmentation\n",
    "    'augmentation_prob': 0.3,\n",
    "    'time_stretch_range': [0.8, 1.2],\n",
    "    'pitch_shift_range': [-2, 2],\n",
    "    'noise_level': 0.005\n",
    "}\n",
    "\n",
    "print(f\"‚öôÔ∏è  Configuration loaded: {CONFIG['batch_size']} batch size, {CONFIG['epochs']} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149e767",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "Loading the raga dataset and preparing separate preprocessing pipelines for AST and VGGish models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd79c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Dataset paths\n",
    "DATASET_PATH = \"/content/drive/MyDrive/Raga_Dataset\"  # Update this path\n",
    "OUTPUT_PATH = \"/content/drive/MyDrive/AST_VGGish_Output\"\n",
    "MODELS_PATH = \"/content/drive/MyDrive/AST_VGGish_Models\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Dataset path: {DATASET_PATH}\")\n",
    "print(f\"üíæ Models path: {MODELS_PATH}\")\n",
    "print(f\"üìä Output path: {OUTPUT_PATH}\")\n",
    "\n",
    "def discover_audio_dataset(dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Discover and catalog all audio files in the dataset.\n",
    "    \"\"\"\n",
    "    audio_files = []\n",
    "    supported_formats = ['.wav', '.mp3', '.flac', '.m4a', '.ogg']\n",
    "    \n",
    "    print(\"üîç Discovering audio files...\")\n",
    "    \n",
    "    for raga_folder in os.listdir(dataset_path):\n",
    "        raga_path = os.path.join(dataset_path, raga_folder)\n",
    "        \n",
    "        if not os.path.isdir(raga_path):\n",
    "            continue\n",
    "            \n",
    "        for audio_file in os.listdir(raga_path):\n",
    "            file_path = os.path.join(raga_path, audio_file)\n",
    "            file_ext = os.path.splitext(audio_file)[1].lower()\n",
    "            \n",
    "            if file_ext in supported_formats:\n",
    "                try:\n",
    "                    # Get basic audio info\n",
    "                    info = sf.info(file_path)\n",
    "                    \n",
    "                    audio_files.append({\n",
    "                        'file_path': file_path,\n",
    "                        'filename': audio_file,\n",
    "                        'raga': raga_folder,\n",
    "                        'duration': info.duration,\n",
    "                        'sample_rate': info.samplerate,\n",
    "                        'channels': info.channels,\n",
    "                        'format': file_ext[1:],\n",
    "                        'file_size_mb': os.path.getsize(file_path) / (1024 * 1024)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Error reading {file_path}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(audio_files)\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(df)} audio files\")\n",
    "    print(f\"üìä Raga classes: {df['raga'].nunique()}\")\n",
    "    print(f\"üéµ Total duration: {df['duration'].sum()/3600:.2f} hours\")\n",
    "    \n",
    "    # Display class distribution\n",
    "    print(f\"\\\\nüìà Class distribution:\")\n",
    "    class_counts = df['raga'].value_counts()\n",
    "    for raga, count in class_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   {raga}: {count} files ({percentage:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Discover dataset\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    metadata_df = discover_audio_dataset(DATASET_PATH)\n",
    "    \n",
    "    # Create label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    metadata_df['label'] = label_encoder.fit_transform(metadata_df['raga'])\n",
    "    \n",
    "    # Save label encoder\n",
    "    label_encoder_path = os.path.join(OUTPUT_PATH, 'label_encoder.pkl')\n",
    "    with open(label_encoder_path, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    print(f\"\\\\nüíæ Label encoder saved: {label_encoder_path}\")\n",
    "    print(f\"üè∑Ô∏è  Classes: {list(label_encoder.classes_)}\")\n",
    "    \n",
    "    NUM_CLASSES = len(label_encoder.classes_)\n",
    "    print(f\"üî¢ Number of classes: {NUM_CLASSES}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please update the DATASET_PATH variable with your correct Google Drive path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ca3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "def create_data_splits(df: pd.DataFrame, test_size: float = 0.2, val_size: float = 0.2, random_state: int = 42):\n",
    "    \"\"\"Create stratified train/validation/test splits.\"\"\"\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    train_val, test = train_test_split(\n",
    "        df, test_size=test_size, stratify=df['label'], random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    train, val = train_test_split(\n",
    "        train_val, test_size=val_size_adjusted, stratify=train_val['label'], random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Dataset splits:\")\n",
    "    print(f\"   Train: {len(train)} files ({len(train)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Validation: {len(val)} files ({len(val)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Test: {len(test)} files ({len(test)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "# Audio preprocessing utilities\n",
    "class AudioAugmentation:\n",
    "    \"\"\"Audio augmentation pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate: int = 16000):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.augment = Compose([\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=CONFIG['noise_level'], p=0.3),\n",
    "            TimeStretch(min_rate=CONFIG['time_stretch_range'][0], \n",
    "                       max_rate=CONFIG['time_stretch_range'][1], p=0.3),\n",
    "            PitchShift(min_semitones=CONFIG['pitch_shift_range'][0],\n",
    "                      max_semitones=CONFIG['pitch_shift_range'][1], p=0.3),\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, audio: np.ndarray, apply_augmentation: bool = True) -> np.ndarray:\n",
    "        if apply_augmentation and random.random() < CONFIG['augmentation_prob']:\n",
    "            try:\n",
    "                return self.augment(samples=audio, sample_rate=self.sample_rate)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Augmentation failed: {e}\")\n",
    "        return audio\n",
    "\n",
    "def load_and_preprocess_audio(file_path: str, target_sr: int = CONFIG['sample_rate']) -> np.ndarray:\n",
    "    \"\"\"Load and preprocess audio file.\"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "        audio = librosa.util.normalize(audio)\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading {file_path}: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def prepare_ast_spectrogram(audio: np.ndarray, \n",
    "                           feature_extractor: ASTFeatureExtractor) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Prepare mel spectrogram for AST model.\"\"\"\n",
    "    try:\n",
    "        # AST feature extractor expects audio as list or numpy array\n",
    "        inputs = feature_extractor(\n",
    "            audio, \n",
    "            sampling_rate=CONFIG['sample_rate'],\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=CONFIG['ast_max_length'],\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        return inputs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"AST preprocessing error: {e}\")\n",
    "        return {\"input_values\": torch.zeros(1, CONFIG['ast_max_length'])}\n",
    "\n",
    "def extract_vggish_embeddings(audio: np.ndarray, vggish_model) -> np.ndarray:\n",
    "    \"\"\"Extract VGGish embeddings from audio.\"\"\"\n",
    "    try:\n",
    "        # VGGish expects audio in specific format\n",
    "        # Reshape audio to match VGGish input requirements\n",
    "        if len(audio.shape) == 1:\n",
    "            audio = audio.reshape(1, -1)\n",
    "        \n",
    "        # Extract embeddings\n",
    "        embeddings = vggish_model(audio)\n",
    "        \n",
    "        # Average embeddings if multiple time steps\n",
    "        if len(embeddings.shape) > 1 and embeddings.shape[0] > 1:\n",
    "            embeddings = tf.reduce_mean(embeddings, axis=0, keepdims=True)\n",
    "        \n",
    "        return embeddings.numpy()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"VGGish preprocessing error: {e}\")\n",
    "        return np.zeros((1, CONFIG['vggish_embedding_dim']))\n",
    "\n",
    "# Create data splits\n",
    "if 'metadata_df' in locals() and len(metadata_df) > 0:\n",
    "    train_df, val_df, test_df = create_data_splits(metadata_df)\n",
    "    \n",
    "    # Initialize augmentation\n",
    "    audio_augmenter = AudioAugmentation()\n",
    "    \n",
    "    print(f\"‚úÖ Data splits and preprocessing pipeline ready!\")\n",
    "    print(f\"üìä Augmentation probability: {CONFIG['augmentation_prob']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No metadata available. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd946b",
   "metadata": {},
   "source": [
    "## 3. Audio Spectrogram Transformer (AST) Implementation\n",
    "\n",
    "Loading and fine-tuning the AST model for raga classification using PyTorch and Hugging Face transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AST Model Configuration\n",
    "AST_MODEL_NAME = \"MIT/ast-finetuned-audioset-10-10-0.4593\"  # Pre-trained AST model\n",
    "\n",
    "class ASTDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for AST model.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe: pd.DataFrame, feature_extractor: ASTFeatureExtractor, \n",
    "                 augment: bool = False, augmenter: Optional[AudioAugmentation] = None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.augment = augment\n",
    "        self.augmenter = augmenter\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load audio\n",
    "        audio = load_and_preprocess_audio(row['file_path'])\n",
    "        \n",
    "        if len(audio) == 0:\n",
    "            # Return zero tensor if loading failed\n",
    "            return {\n",
    "                'input_values': torch.zeros(CONFIG['ast_max_length']),\n",
    "                'labels': torch.tensor(row['label'], dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment and self.augmenter:\n",
    "            audio = self.augmenter(audio, apply_augmentation=True)\n",
    "        \n",
    "        # Convert to AST input format\n",
    "        inputs = prepare_ast_spectrogram(audio, self.feature_extractor)\n",
    "        \n",
    "        # Prepare output\n",
    "        result = {\n",
    "            'input_values': inputs['input_values'].squeeze(0),  # Remove batch dimension\n",
    "            'labels': torch.tensor(row['label'], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "def setup_ast_model(num_classes: int):\n",
    "    \"\"\"Setup AST model and feature extractor.\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Loading AST model: {AST_MODEL_NAME}\")\n",
    "    \n",
    "    try:\n",
    "        # Load feature extractor\n",
    "        feature_extractor = ASTFeatureExtractor.from_pretrained(AST_MODEL_NAME)\n",
    "        \n",
    "        # Load model with custom classification head\n",
    "        model = ASTForAudioClassification.from_pretrained(\n",
    "            AST_MODEL_NAME,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        model = model.to(DEVICE)\n",
    "        \n",
    "        print(f\"‚úÖ AST model loaded successfully!\")\n",
    "        print(f\"   Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "        print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.1f}M\")\n",
    "        \n",
    "        return model, feature_extractor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load AST model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def create_ast_data_loaders(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                           feature_extractor: ASTFeatureExtractor, batch_size: int = CONFIG['batch_size']):\n",
    "    \"\"\"Create PyTorch data loaders for AST training.\"\"\"\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ASTDataset(train_df, feature_extractor, augment=True, augmenter=audio_augmenter)\n",
    "    val_dataset = ASTDataset(val_df, feature_extractor, augment=False)\n",
    "    test_dataset = ASTDataset(test_df, feature_extractor, augment=False)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä AST Data loaders created:\")\n",
    "    print(f\"   Train: {len(train_loader)} batches ({len(train_dataset)} samples)\")\n",
    "    print(f\"   Validation: {len(val_loader)} batches ({len(val_dataset)} samples)\")\n",
    "    print(f\"   Test: {len(test_loader)} batches ({len(test_dataset)} samples)\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Setup AST model and data loaders\n",
    "if all(var in locals() for var in ['NUM_CLASSES', 'train_df', 'val_df', 'test_df']):\n",
    "    ast_model, ast_feature_extractor = setup_ast_model(NUM_CLASSES)\n",
    "    \n",
    "    if ast_model is not None:\n",
    "        # Create data loaders\n",
    "        ast_train_loader, ast_val_loader, ast_test_loader = create_ast_data_loaders(\n",
    "            train_df, val_df, test_df, ast_feature_extractor\n",
    "        )\n",
    "        \n",
    "        # Test data loading\n",
    "        print(\"\\\\nüß™ Testing AST data loading...\")\n",
    "        try:\n",
    "            batch = next(iter(ast_train_loader))\n",
    "            print(f\"   Input shape: {batch['input_values'].shape}\")\n",
    "            print(f\"   Labels shape: {batch['labels'].shape}\")\n",
    "            print(f\"   Sample input range: [{batch['input_values'].min():.3f}, {batch['input_values'].max():.3f}]\")\n",
    "            print(\"‚úÖ AST data loading test successful!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå AST data loading test failed: {e}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Missing required variables for AST setup. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64700ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AST Training Configuration\n",
    "def setup_ast_training(model, train_loader, val_loader, output_dir: str):\n",
    "    \"\"\"Setup AST training with Hugging Face Trainer.\"\"\"\n",
    "    \n",
    "    # Create output directory for AST\n",
    "    ast_output_dir = os.path.join(output_dir, \"ast_model\")\n",
    "    os.makedirs(ast_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert PyTorch DataLoaders to HuggingFace Dataset format\n",
    "    def create_hf_dataset(loader):\n",
    "        all_inputs = []\n",
    "        all_labels = []\n",
    "        \n",
    "        print(f\"Converting data loader to HuggingFace format...\")\n",
    "        for batch in tqdm(loader):\n",
    "            all_inputs.extend(batch['input_values'].cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        return HFDataset.from_dict({\n",
    "            'input_values': all_inputs,\n",
    "            'labels': all_labels\n",
    "        })\n",
    "    \n",
    "    # Convert datasets\n",
    "    print(\"üîÑ Converting train dataset...\")\n",
    "    hf_train_dataset = create_hf_dataset(train_loader)\n",
    "    print(\"üîÑ Converting validation dataset...\")\n",
    "    hf_val_dataset = create_hf_dataset(val_loader)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=ast_output_dir,\n",
    "        num_train_epochs=CONFIG['epochs'],\n",
    "        per_device_train_batch_size=CONFIG['batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "        gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "        learning_rate=CONFIG['learning_rate_ast'],\n",
    "        weight_decay=CONFIG['weight_decay'],\n",
    "        warmup_steps=CONFIG['warmup_steps'],\n",
    "        max_grad_norm=CONFIG['max_grad_norm'],\n",
    "        \n",
    "        # Evaluation and logging\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        logging_dir=os.path.join(ast_output_dir, \"logs\"),\n",
    "        logging_steps=50,\n",
    "        \n",
    "        # Early stopping and checkpointing\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=3,\n",
    "        \n",
    "        # Performance optimization\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=2,\n",
    "        fp16=torch.cuda.is_available(),  # Enable mixed precision if CUDA available\n",
    "        \n",
    "        # Other settings\n",
    "        seed=42,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    # Metrics computation\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = (predictions == labels).mean()\n",
    "        f1_macro = f1_score(labels, predictions, average='macro')\n",
    "        f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_weighted': f1_weighted\n",
    "        }\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=hf_train_dataset,\n",
    "        eval_dataset=hf_val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG['patience'])]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ AST Trainer configured!\")\n",
    "    print(f\"üìÅ Output directory: {ast_output_dir}\")\n",
    "    print(f\"üéØ Training configuration:\")\n",
    "    print(f\"   Learning rate: {CONFIG['learning_rate_ast']}\")\n",
    "    print(f\"   Batch size: {CONFIG['batch_size']} (effective: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']})\")\n",
    "    print(f\"   Epochs: {CONFIG['epochs']}\")\n",
    "    print(f\"   Warmup steps: {CONFIG['warmup_steps']}\")\n",
    "    \n",
    "    return trainer, ast_output_dir\n",
    "\n",
    "def train_ast_model(trainer, output_dir: str):\n",
    "    \"\"\"Train the AST model.\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting AST model training...\")\n",
    "    \n",
    "    try:\n",
    "        # Start training\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Save the model\n",
    "        trainer.save_model()\n",
    "        trainer.save_state()\n",
    "        \n",
    "        # Save training metrics\n",
    "        metrics_path = os.path.join(output_dir, \"training_metrics.json\")\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(train_result.metrics, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ AST training completed!\")\n",
    "        print(f\"üìä Final training loss: {train_result.metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "        print(f\"üíæ Model saved to: {output_dir}\")\n",
    "        \n",
    "        return train_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå AST training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Setup and train AST model\n",
    "if all(var in locals() for var in ['ast_model', 'ast_train_loader', 'ast_val_loader']):\n",
    "    \n",
    "    # Setup training\n",
    "    ast_trainer, ast_output_dir = setup_ast_training(\n",
    "        ast_model, ast_train_loader, ast_val_loader, MODELS_PATH\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"üéØ Ready to train AST model!\")\n",
    "    print(\"‚ö†Ô∏è  This will take significant time. Monitor progress in TensorBoard:\")\n",
    "    print(f\"   tensorboard --logdir {ast_output_dir}/logs\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Uncomment the following line to start training\n",
    "    # ast_train_result = train_ast_model(ast_trainer, ast_output_dir)\n",
    "    print(\"\\\\nüí° To start training, uncomment the line above and run the cell.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå AST model setup incomplete. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AST Model Evaluation and Inference\n",
    "def evaluate_ast_model(trainer, test_loader, output_dir: str):\n",
    "    \"\"\"Evaluate the trained AST model on test data.\"\"\"\n",
    "    \n",
    "    print(\"üß™ Evaluating AST model on test set...\")\n",
    "    \n",
    "    # Convert test loader to HF dataset format\n",
    "    test_inputs = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for batch in tqdm(test_loader, desc=\"Preparing test data\"):\n",
    "        test_inputs.extend(batch['input_values'].cpu().numpy())\n",
    "        test_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    hf_test_dataset = HFDataset.from_dict({\n",
    "        'input_values': test_inputs,\n",
    "        'labels': test_labels\n",
    "    })\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_results = trainer.evaluate(eval_dataset=hf_test_dataset)\n",
    "    \n",
    "    # Generate detailed classification report\n",
    "    predictions = trainer.predict(hf_test_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_true = predictions.label_ids\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, \n",
    "        target_names=CLASS_NAMES, \n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Save evaluation results\n",
    "    eval_file = os.path.join(output_dir, \"ast_evaluation_results.json\")\n",
    "    with open(eval_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'eval_metrics': eval_results,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'class_names': CLASS_NAMES\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Confusion Matrix\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "    plt.title('AST Model - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Plot 2: Per-class F1 scores\n",
    "    plt.subplot(1, 3, 2)\n",
    "    f1_scores = [report[cls]['f1-score'] for cls in CLASS_NAMES]\n",
    "    plt.bar(range(len(CLASS_NAMES)), f1_scores, color='skyblue')\n",
    "    plt.title('AST Model - F1 Score per Class')\n",
    "    plt.xlabel('Raga Classes')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xticks(range(len(CLASS_NAMES)), CLASS_NAMES, rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot 3: Precision vs Recall\n",
    "    plt.subplot(1, 3, 3)\n",
    "    precisions = [report[cls]['precision'] for cls in CLASS_NAMES]\n",
    "    recalls = [report[cls]['recall'] for cls in CLASS_NAMES]\n",
    "    plt.scatter(recalls, precisions, color='red', alpha=0.7)\n",
    "    for i, cls in enumerate(CLASS_NAMES):\n",
    "        plt.annotate(cls, (recalls[i], precisions[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('AST Model - Precision vs Recall')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'ast_evaluation_plots.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\\\nüìä AST Model Evaluation Results:\")\n",
    "    print(f\"   Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"   F1 (Macro): {eval_results['eval_f1_macro']:.4f}\")\n",
    "    print(f\"   F1 (Weighted): {eval_results['eval_f1_weighted']:.4f}\")\n",
    "    print(f\"üíæ Results saved to: {eval_file}\")\n",
    "    \n",
    "    return eval_results, report, cm\n",
    "\n",
    "def predict_raga_ast(model, processor, audio_path: str, device):\n",
    "    \"\"\"Predict raga for a single audio file using AST model.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess audio\n",
    "        waveform, sr = librosa.load(audio_path, sr=CONFIG['sample_rate'])\n",
    "        \n",
    "        # Apply augmentation/preprocessing\n",
    "        if len(waveform) > CONFIG['max_length'] * sr:\n",
    "            waveform = waveform[:CONFIG['max_length'] * sr]\n",
    "        else:\n",
    "            waveform = np.pad(waveform, (0, max(0, CONFIG['max_length'] * sr - len(waveform))))\n",
    "        \n",
    "        # Process with AST processor\n",
    "        inputs = processor(waveform, sampling_rate=sr, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return {\n",
    "            'predicted_raga': CLASS_NAMES[predicted_class],\n",
    "            'confidence': confidence,\n",
    "            'all_probabilities': {\n",
    "                CLASS_NAMES[i]: prob.item() \n",
    "                for i, prob in enumerate(probabilities[0])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Prediction failed for {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example evaluation (when model is trained)\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"üìä AST Model Evaluation Ready\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\\\nüí° After training completes, run evaluation with:\")\n",
    "print(\"ast_eval_results, ast_report, ast_cm = evaluate_ast_model(ast_trainer, ast_test_loader, ast_output_dir)\")\n",
    "print(\"\\\\nüéØ For single audio prediction:\")\n",
    "print(\"prediction = predict_raga_ast(ast_model, ast_processor, 'path/to/audio.wav', device)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8a810",
   "metadata": {},
   "source": [
    "## 5. VGGish Model Pipeline\n",
    "\n",
    "### VGGish Overview\n",
    "VGGish is a CNN-based audio feature extractor pre-trained on YouTube-8M dataset. It processes audio in log-mel spectrogram format and outputs 128-dimensional embeddings. We'll use TensorFlow Hub's VGGish model and add a classification head for raga classification.\n",
    "\n",
    "**Key Features:**\n",
    "- Pre-trained CNN feature extractor\n",
    "- Fixed 128-dimensional embeddings\n",
    "- Mel-spectrogram input preprocessing\n",
    "- Suitable for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGish Configuration and Preprocessing\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# VGGish specific configuration\n",
    "VGGISH_CONFIG = {\n",
    "    'model_url': 'https://tfhub.dev/google/vggish/1',\n",
    "    'sample_rate': 16000,\n",
    "    'stft_window_seconds': 0.025,\n",
    "    'stft_hop_seconds': 0.010,\n",
    "    'mel_bands': 64,\n",
    "    'mel_min_hz': 125,\n",
    "    'mel_max_hz': 7500,\n",
    "    'log_offset': 0.01,\n",
    "    'example_window_seconds': 0.96,\n",
    "    'example_hop_seconds': 0.96,\n",
    "    'embedding_size': 128\n",
    "}\n",
    "\n",
    "print(\"üîß VGGish Configuration:\")\n",
    "for key, value in VGGISH_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "def vggish_preprocess_audio(waveform, sample_rate=VGGISH_CONFIG['sample_rate']):\n",
    "    \"\"\"\n",
    "    Preprocess audio for VGGish model.\n",
    "    Converts audio to log-mel spectrograms as expected by VGGish.\n",
    "    \"\"\"\n",
    "    # Ensure correct sample rate\n",
    "    if len(waveform.shape) > 1:\n",
    "        waveform = waveform.mean(axis=0)  # Convert to mono if stereo\n",
    "    \n",
    "    # Convert to float32 and normalize\n",
    "    waveform = waveform.astype(np.float32)\n",
    "    waveform = waveform / np.max(np.abs(waveform) + 1e-8)  # Normalize\n",
    "    \n",
    "    # Parameters for STFT\n",
    "    window_length_samples = int(round(sample_rate * VGGISH_CONFIG['stft_window_seconds']))\n",
    "    hop_length_samples = int(round(sample_rate * VGGISH_CONFIG['stft_hop_seconds']))\n",
    "    fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n",
    "    \n",
    "    # Compute STFT\n",
    "    stft = librosa.stft(waveform,\n",
    "                       hop_length=hop_length_samples,\n",
    "                       win_length=window_length_samples,\n",
    "                       n_fft=fft_length,\n",
    "                       center=False)\n",
    "    \n",
    "    # Convert to magnitude spectrogram\n",
    "    magnitude_spectrogram = np.abs(stft)\n",
    "    \n",
    "    # Convert to mel-scale\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        S=magnitude_spectrogram**2,\n",
    "        sr=sample_rate,\n",
    "        n_mels=VGGISH_CONFIG['mel_bands'],\n",
    "        fmin=VGGISH_CONFIG['mel_min_hz'],\n",
    "        fmax=VGGISH_CONFIG['mel_max_hz']\n",
    "    )\n",
    "    \n",
    "    # Convert to log scale\n",
    "    log_mel_spectrogram = np.log(mel_spectrogram + VGGISH_CONFIG['log_offset'])\n",
    "    \n",
    "    # Extract fixed-size windows\n",
    "    window_length_frames = int(round(VGGISH_CONFIG['example_window_seconds'] / VGGISH_CONFIG['stft_hop_seconds']))\n",
    "    hop_length_frames = int(round(VGGISH_CONFIG['example_hop_seconds'] / VGGISH_CONFIG['stft_hop_seconds']))\n",
    "    \n",
    "    # Extract examples (patches)\n",
    "    examples = []\n",
    "    for i in range(0, log_mel_spectrogram.shape[1] - window_length_frames + 1, hop_length_frames):\n",
    "        examples.append(log_mel_spectrogram[:, i:i + window_length_frames])\n",
    "    \n",
    "    if len(examples) == 0:\n",
    "        # If audio is too short, pad the spectrogram\n",
    "        padded_spectrogram = np.pad(log_mel_spectrogram, \n",
    "                                  ((0, 0), (0, window_length_frames - log_mel_spectrogram.shape[1])), \n",
    "                                  mode='constant')\n",
    "        examples = [padded_spectrogram]\n",
    "    \n",
    "    return np.array(examples)\n",
    "\n",
    "# VGGish Dataset Class\n",
    "class VGGishDataset:\n",
    "    \"\"\"Dataset class for VGGish model preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, audio_files, labels, is_training=True):\n",
    "        self.audio_files = audio_files\n",
    "        self.labels = labels\n",
    "        self.is_training = is_training\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def preprocess_audio(self, audio_path):\n",
    "        \"\"\"Load and preprocess audio for VGGish.\"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            waveform, sr = librosa.load(audio_path, sr=VGGISH_CONFIG['sample_rate'])\n",
    "            \n",
    "            # Apply preprocessing for VGGish\n",
    "            examples = vggish_preprocess_audio(waveform, sr)\n",
    "            \n",
    "            # During training, we can use data augmentation\n",
    "            if self.is_training and len(examples) > 1:\n",
    "                # Randomly select one example during training\n",
    "                idx = np.random.randint(0, len(examples))\n",
    "                return examples[idx]\n",
    "            else:\n",
    "                # During inference, average all examples\n",
    "                return np.mean(examples, axis=0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "            # Return zero spectrogram if loading fails\n",
    "            return np.zeros((VGGISH_CONFIG['mel_bands'], \n",
    "                           int(round(VGGISH_CONFIG['example_window_seconds'] / VGGISH_CONFIG['stft_hop_seconds']))))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Preprocess audio\n",
    "        spectrogram = self.preprocess_audio(audio_path)\n",
    "        \n",
    "        return {\n",
    "            'spectrogram': spectrogram.astype(np.float32),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "def create_vggish_data_generators(train_files, train_labels, val_files, val_labels, test_files, test_labels, batch_size=32):\n",
    "    \"\"\"Create TensorFlow data generators for VGGish training.\"\"\"\n",
    "    \n",
    "    def data_generator(files, labels, is_training=True):\n",
    "        \"\"\"Generator function for TensorFlow dataset.\"\"\"\n",
    "        dataset = VGGishDataset(files, labels, is_training)\n",
    "        \n",
    "        for i in range(len(dataset)):\n",
    "            item = dataset[i]\n",
    "            yield item['spectrogram'], item['label']\n",
    "    \n",
    "    # Define output signature\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(VGGISH_CONFIG['mel_bands'], \n",
    "                           int(round(VGGISH_CONFIG['example_window_seconds'] / VGGISH_CONFIG['stft_hop_seconds']))), \n",
    "                     dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(train_files, train_labels, True),\n",
    "        output_signature=output_signature\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(val_files, val_labels, False),\n",
    "        output_signature=output_signature\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(test_files, test_labels, False),\n",
    "        output_signature=output_signature\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    print(f\"‚úÖ VGGish data generators created!\")\n",
    "    print(f\"   Train batches: {len(list(train_dataset))}\")\n",
    "    print(f\"   Validation batches: {len(list(val_dataset))}\")\n",
    "    print(f\"   Test batches: {len(list(test_dataset))}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Create VGGish datasets\n",
    "if 'train_files' in locals() and 'train_labels' in locals():\n",
    "    print(\"\\\\nüîÑ Creating VGGish data generators...\")\n",
    "    vggish_train_dataset, vggish_val_dataset, vggish_test_dataset = create_vggish_data_generators(\n",
    "        train_files, train_labels,\n",
    "        val_files, val_labels, \n",
    "        test_files, test_labels,\n",
    "        batch_size=CONFIG['batch_size']\n",
    "    )\n",
    "    print(\"‚úÖ VGGish datasets ready!\")\n",
    "else:\n",
    "    print(\"‚ùå Data files not available. Please run data loading cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGish Model Architecture\n",
    "def create_vggish_model(num_classes: int):\n",
    "    \"\"\"\n",
    "    Create VGGish model with classification head.\n",
    "    \n",
    "    Architecture:\n",
    "    1. VGGish feature extractor (frozen/fine-tunable)\n",
    "    2. Classification head with dropout and batch norm\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pre-trained VGGish model from TensorFlow Hub\n",
    "    print(\"üîÑ Loading VGGish model from TensorFlow Hub...\")\n",
    "    vggish_layer = hub.KerasLayer(\n",
    "        VGGISH_CONFIG['model_url'],\n",
    "        input_shape=(VGGISH_CONFIG['mel_bands'], \n",
    "                    int(round(VGGISH_CONFIG['example_window_seconds'] / VGGISH_CONFIG['stft_hop_seconds']))),\n",
    "        trainable=False,  # Start with frozen features\n",
    "        name='vggish'\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    inputs = layers.Input(shape=(VGGISH_CONFIG['mel_bands'], \n",
    "                                int(round(VGGISH_CONFIG['example_window_seconds'] / VGGISH_CONFIG['stft_hop_seconds']))),\n",
    "                         name='mel_spectrogram')\n",
    "    \n",
    "    # VGGish feature extraction\n",
    "    features = vggish_layer(inputs)  # Output: (batch_size, 128)\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.BatchNormalization(name='bn1')(features)\n",
    "    x = layers.Dropout(0.5, name='dropout1')(x)\n",
    "    x = layers.Dense(256, activation='relu', name='dense1')(x)\n",
    "    x = layers.BatchNormalization(name='bn2')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout2')(x)\n",
    "    x = layers.Dense(128, activation='relu', name='dense2')(x)\n",
    "    x = layers.BatchNormalization(name='bn3')(x)\n",
    "    x = layers.Dropout(0.2, name='dropout3')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='classification')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='VGGish_RagaClassifier')\n",
    "    \n",
    "    return model, vggish_layer\n",
    "\n",
    "def setup_vggish_training(model, vggish_layer):\n",
    "    \"\"\"Setup VGGish model for training with different strategies.\"\"\"\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=CONFIG['learning_rate_vggish']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"üéØ VGGish Training Strategy:\")\n",
    "    print(\"   Phase 1: Frozen VGGish features + Train classification head\")\n",
    "    print(\"   Phase 2: Fine-tune VGGish features + classification head\")\n",
    "    \n",
    "    # Training callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=CONFIG['patience'],\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=CONFIG['patience']//2,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(MODELS_PATH, 'vggish_model', 'best_model.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "def train_vggish_two_phase(model, vggish_layer, train_dataset, val_dataset, callbacks):\n",
    "    \"\"\"Train VGGish model in two phases: frozen then fine-tuning.\"\"\"\n",
    "    \n",
    "    vggish_output_dir = os.path.join(MODELS_PATH, \"vggish_model\")\n",
    "    os.makedirs(vggish_output_dir, exist_ok=True)\n",
    "    \n",
    "    training_history = {'phase1': None, 'phase2': None}\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ PHASE 1: Training classification head (VGGish frozen)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Phase 1: Train only classification head\n",
    "    vggish_layer.trainable = False\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=CONFIG['learning_rate_vggish']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train Phase 1\n",
    "    history1 = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=CONFIG['epochs'] // 2,  # Half epochs for phase 1\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_history['phase1'] = history1.history\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"üî• PHASE 2: Fine-tuning entire model (VGGish unfrozen)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Phase 2: Fine-tune entire model\n",
    "    vggish_layer.trainable = True\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=CONFIG['learning_rate_vggish'] * 0.1),  # Lower LR for fine-tuning\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train Phase 2\n",
    "    history2 = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=CONFIG['epochs'] // 2,  # Remaining epochs for phase 2\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        initial_epoch=len(history1.history['loss'])  # Continue from phase 1\n",
    "    )\n",
    "    training_history['phase2'] = history2.history\n",
    "    \n",
    "    # Save complete training history\n",
    "    history_path = os.path.join(vggish_output_dir, \"training_history.json\")\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    \n",
    "    # Save final model\n",
    "    model.save(os.path.join(vggish_output_dir, \"final_model.h5\"))\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ VGGish training completed!\")\n",
    "    print(f\"üíæ Model saved to: {vggish_output_dir}\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "# Setup VGGish model\n",
    "if 'CLASS_NAMES' in locals():\n",
    "    print(\"\\\\nüîÑ Setting up VGGish model...\")\n",
    "    \n",
    "    # Create model\n",
    "    vggish_model, vggish_feature_layer = create_vggish_model(len(CLASS_NAMES))\n",
    "    \n",
    "    # Setup training\n",
    "    vggish_callbacks = setup_vggish_training(vggish_model, vggish_feature_layer)\n",
    "    \n",
    "    # Model summary\n",
    "    print(\"\\\\nüìã VGGish Model Architecture:\")\n",
    "    vggish_model.summary()\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ VGGish model ready!\")\n",
    "    print(f\"üìä Total parameters: {vggish_model.count_params():,}\")\n",
    "    print(f\"üéØ Output classes: {len(CLASS_NAMES)}\")\n",
    "    \n",
    "    # Visualize model architecture\n",
    "    tf.keras.utils.plot_model(\n",
    "        vggish_model, \n",
    "        to_file=os.path.join(MODELS_PATH, 'vggish_architecture.png'),\n",
    "        show_shapes=True, \n",
    "        show_layer_names=True,\n",
    "        dpi=150\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\nüí° To start training:\")\n",
    "    print(\"vggish_history = train_vggish_two_phase(vggish_model, vggish_feature_layer, vggish_train_dataset, vggish_val_dataset, vggish_callbacks)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå CLASS_NAMES not defined. Please run data loading cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa32416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGish Model Evaluation\n",
    "def evaluate_vggish_model(model, test_dataset, output_dir: str):\n",
    "    \"\"\"Evaluate trained VGGish model on test data.\"\"\"\n",
    "    \n",
    "    print(\"üß™ Evaluating VGGish model on test set...\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_results = model.evaluate(test_dataset, verbose=1)\n",
    "    test_loss, test_accuracy = test_results\n",
    "    \n",
    "    # Get predictions for detailed analysis\n",
    "    print(\"üîÑ Generating predictions for detailed analysis...\")\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for batch_spectrograms, batch_labels in tqdm(test_dataset):\n",
    "        # Get predictions\n",
    "        predictions = model.predict(batch_spectrograms, verbose=0)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        y_true.extend(batch_labels.numpy())\n",
    "        y_pred.extend(predicted_classes)\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=CLASS_NAMES,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Save evaluation results\n",
    "    eval_file = os.path.join(output_dir, \"vggish_evaluation_results.json\")\n",
    "    with open(eval_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'test_loss': float(test_loss),\n",
    "            'test_accuracy': float(test_accuracy),\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'class_names': CLASS_NAMES\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Confusion Matrix\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',\n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "    plt.title('VGGish Model - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Plot 2: Per-class F1 scores\n",
    "    plt.subplot(1, 3, 2)\n",
    "    f1_scores = [report[cls]['f1-score'] for cls in CLASS_NAMES]\n",
    "    plt.bar(range(len(CLASS_NAMES)), f1_scores, color='orange', alpha=0.7)\n",
    "    plt.title('VGGish Model - F1 Score per Class')\n",
    "    plt.xlabel('Raga Classes')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xticks(range(len(CLASS_NAMES)), CLASS_NAMES, rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot 3: Precision vs Recall\n",
    "    plt.subplot(1, 3, 3)\n",
    "    precisions = [report[cls]['precision'] for cls in CLASS_NAMES]\n",
    "    recalls = [report[cls]['recall'] for cls in CLASS_NAMES]\n",
    "    plt.scatter(recalls, precisions, color='darkorange', alpha=0.7)\n",
    "    for i, cls in enumerate(CLASS_NAMES):\n",
    "        plt.annotate(cls, (recalls[i], precisions[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('VGGish Model - Precision vs Recall')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'vggish_evaluation_plots.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\\\nüìä VGGish Model Evaluation Results:\")\n",
    "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"   F1 (Macro): {report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"   F1 (Weighted): {report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(f\"üíæ Results saved to: {eval_file}\")\n",
    "    \n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def predict_raga_vggish(model, audio_path: str):\n",
    "    \"\"\"Predict raga for a single audio file using VGGish model.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess audio\n",
    "        waveform, sr = librosa.load(audio_path, sr=VGGISH_CONFIG['sample_rate'])\n",
    "        \n",
    "        # Apply VGGish preprocessing\n",
    "        spectrograms = vggish_preprocess_audio(waveform, sr)\n",
    "        \n",
    "        # Average predictions across all spectrograms\n",
    "        all_predictions = []\n",
    "        for spectrogram in spectrograms:\n",
    "            # Expand dimensions for batch\n",
    "            input_spectrogram = np.expand_dims(spectrogram, axis=0)\n",
    "            \n",
    "            # Predict\n",
    "            prediction = model.predict(input_spectrogram, verbose=0)\n",
    "            all_predictions.append(prediction[0])\n",
    "        \n",
    "        # Average predictions\n",
    "        avg_prediction = np.mean(all_predictions, axis=0)\n",
    "        predicted_class = np.argmax(avg_prediction)\n",
    "        confidence = avg_prediction[predicted_class]\n",
    "        \n",
    "        return {\n",
    "            'predicted_raga': CLASS_NAMES[predicted_class],\n",
    "            'confidence': float(confidence),\n",
    "            'all_probabilities': {\n",
    "                CLASS_NAMES[i]: float(prob)\n",
    "                for i, prob in enumerate(avg_prediction)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå VGGish prediction failed for {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example evaluation setup\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"üìä VGGish Model Evaluation Ready\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\\\nüí° After training completes, run evaluation with:\")\n",
    "print(\"vggish_eval_results = evaluate_vggish_model(vggish_model, vggish_test_dataset, os.path.join(MODELS_PATH, 'vggish_model'))\")\n",
    "print(\"\\\\nüéØ For single audio prediction:\")\n",
    "print(\"prediction = predict_raga_vggish(vggish_model, 'path/to/audio.wav')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308ccde",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Analysis\n",
    "\n",
    "### Comparative Analysis\n",
    "This section provides comprehensive comparison between AST and VGGish models across multiple dimensions:\n",
    "\n",
    "**Comparison Metrics:**\n",
    "- Performance: Accuracy, F1-scores, Precision, Recall\n",
    "- Computational: Training time, inference speed, model size\n",
    "- Architectural: Parameter count, memory usage\n",
    "- Robustness: Cross-validation results, error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e332d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def compare_models(ast_results=None, vggish_results=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Comprehensive comparison between AST and VGGish models.\n",
    "    \n",
    "    Args:\n",
    "        ast_results: Dictionary containing AST evaluation results\n",
    "        vggish_results: Dictionary containing VGGish evaluation results\n",
    "        save_path: Path to save comparison results\n",
    "    \"\"\"\n",
    "    \n",
    "    if save_path is None:\n",
    "        save_path = MODELS_PATH\n",
    "    \n",
    "    comparison_results = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'models': {}\n",
    "    }\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"üìä COMPREHENSIVE MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Model information\n",
    "    models_info = {\n",
    "        'AST': {\n",
    "            'name': 'Audio Spectrogram Transformer',\n",
    "            'type': 'Transformer-based',\n",
    "            'pre_training': 'AudioSet',\n",
    "            'input_type': 'Raw audio waveform',\n",
    "            'architecture': 'Vision Transformer adapted for audio'\n",
    "        },\n",
    "        'VGGish': {\n",
    "            'name': 'VGGish',\n",
    "            'type': 'CNN-based',\n",
    "            'pre_training': 'YouTube-8M',\n",
    "            'input_type': 'Log-mel spectrogram',\n",
    "            'architecture': 'VGG-like CNN with classification head'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"\\\\nüéØ PERFORMANCE COMPARISON\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    performance_data = []\n",
    "    model_names = []\n",
    "    \n",
    "    if ast_results:\n",
    "        ast_acc = ast_results.get('eval_accuracy', 0)\n",
    "        ast_f1 = ast_results.get('eval_f1_macro', 0)\n",
    "        performance_data.append([ast_acc, ast_f1])\n",
    "        model_names.append('AST')\n",
    "        comparison_results['models']['AST'] = {\n",
    "            'accuracy': float(ast_acc),\n",
    "            'f1_macro': float(ast_f1),\n",
    "            'info': models_info['AST']\n",
    "        }\n",
    "        print(f\"AST Model:\")\n",
    "        print(f\"  Accuracy: {ast_acc:.4f}\")\n",
    "        print(f\"  F1 (Macro): {ast_f1:.4f}\")\n",
    "    \n",
    "    if vggish_results:\n",
    "        vggish_acc = vggish_results.get('test_accuracy', 0)\n",
    "        vggish_f1 = vggish_results['classification_report']['macro avg']['f1-score']\n",
    "        performance_data.append([vggish_acc, vggish_f1])\n",
    "        model_names.append('VGGish')\n",
    "        comparison_results['models']['VGGish'] = {\n",
    "            'accuracy': float(vggish_acc),\n",
    "            'f1_macro': float(vggish_f1),\n",
    "            'info': models_info['VGGish']\n",
    "        }\n",
    "        print(f\"VGGish Model:\")\n",
    "        print(f\"  Accuracy: {vggish_acc:.4f}\")\n",
    "        print(f\"  F1 (Macro): {vggish_f1:.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    if len(performance_data) > 0:\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Performance comparison\n",
    "        plt.subplot(2, 4, 1)\n",
    "        metrics = ['Accuracy', 'F1 Score']\n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        for i, (model_name, data) in enumerate(zip(model_names, performance_data)):\n",
    "            offset = (i - len(model_names)/2 + 0.5) * width\n",
    "            plt.bar(x + offset, data, width, label=model_name, alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Performance Comparison')\n",
    "        plt.xticks(x, metrics)\n",
    "        plt.legend()\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Per-class F1 comparison (if both models available)\n",
    "        if ast_results and vggish_results and 'classification_report' in ast_results:\n",
    "            plt.subplot(2, 4, 2)\n",
    "            \n",
    "            ast_f1_per_class = [ast_results['classification_report'][cls]['f1-score'] for cls in CLASS_NAMES]\n",
    "            vggish_f1_per_class = [vggish_results['classification_report'][cls]['f1-score'] for cls in CLASS_NAMES]\n",
    "            \n",
    "            x = np.arange(len(CLASS_NAMES))\n",
    "            plt.bar(x - 0.2, ast_f1_per_class, 0.4, label='AST', alpha=0.8)\n",
    "            plt.bar(x + 0.2, vggish_f1_per_class, 0.4, label='VGGish', alpha=0.8)\n",
    "            \n",
    "            plt.xlabel('Raga Classes')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.title('Per-Class F1 Score Comparison')\n",
    "            plt.xticks(x, CLASS_NAMES, rotation=45)\n",
    "            plt.legend()\n",
    "            plt.ylim(0, 1)\n",
    "        \n",
    "        # Confusion matrices side by side\n",
    "        subplot_idx = 3\n",
    "        if ast_results and 'confusion_matrix' in ast_results:\n",
    "            plt.subplot(2, 4, subplot_idx)\n",
    "            cm_ast = np.array(ast_results['confusion_matrix'])\n",
    "            sns.heatmap(cm_ast, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cbar=False)\n",
    "            plt.title('AST Confusion Matrix')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.xticks(rotation=45)\n",
    "            subplot_idx += 1\n",
    "        \n",
    "        if vggish_results and 'confusion_matrix' in vggish_results:\n",
    "            plt.subplot(2, 4, subplot_idx)\n",
    "            cm_vggish = np.array(vggish_results['confusion_matrix'])\n",
    "            sns.heatmap(cm_vggish, annot=True, fmt='d', cmap='Oranges',\n",
    "                       xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cbar=False)\n",
    "            plt.title('VGGish Confusion Matrix')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.xticks(rotation=45)\n",
    "            subplot_idx += 1\n",
    "        \n",
    "        # Model architecture comparison\n",
    "        plt.subplot(2, 4, (subplot_idx, subplot_idx + 1))\n",
    "        architecture_comparison = {\n",
    "            'Model': [],\n",
    "            'Type': [],\n",
    "            'Input': [],\n",
    "            'Pre-training': []\n",
    "        }\n",
    "        \n",
    "        for model, info in models_info.items():\n",
    "            if (model == 'AST' and ast_results) or (model == 'VGGish' and vggish_results):\n",
    "                architecture_comparison['Model'].append(info['name'])\n",
    "                architecture_comparison['Type'].append(info['type'])\n",
    "                architecture_comparison['Input'].append(info['input_type'])\n",
    "                architecture_comparison['Pre-training'].append(info['pre_training'])\n",
    "        \n",
    "        # Create table\n",
    "        table_data = list(zip(*[architecture_comparison[key] for key in architecture_comparison.keys()]))\n",
    "        table = plt.table(cellText=table_data,\n",
    "                         colLabels=list(architecture_comparison.keys()),\n",
    "                         cellLoc='center',\n",
    "                         loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 2)\n",
    "        plt.axis('off')\n",
    "        plt.title('Architecture Comparison', pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_path, 'model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Save comprehensive comparison\n",
    "    comparison_file = os.path.join(save_path, 'comprehensive_comparison.json')\n",
    "    with open(comparison_file, 'w') as f:\n",
    "        json.dump(comparison_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\\\nüíæ Comparison results saved to: {comparison_file}\")\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "def benchmark_inference_speed(ast_model=None, vggish_model=None, sample_audio_path=None, num_runs=10):\n",
    "    \"\"\"Benchmark inference speed for both models.\"\"\"\n",
    "    \n",
    "    if sample_audio_path is None:\n",
    "        print(\"‚ùå No sample audio provided for benchmarking\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\\\n‚è±Ô∏è  INFERENCE SPEED BENCHMARK ({num_runs} runs)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # AST benchmark\n",
    "    if ast_model is not None:\n",
    "        print(\"üîÑ Benchmarking AST model...\")\n",
    "        ast_times = []\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            # Simulate AST prediction (replace with actual prediction call)\n",
    "            # prediction = predict_raga_ast(ast_model, ast_processor, sample_audio_path, device)\n",
    "            time.sleep(0.1)  # Placeholder - replace with actual prediction\n",
    "            end_time = time.time()\n",
    "            ast_times.append(end_time - start_time)\n",
    "        \n",
    "        results['AST'] = {\n",
    "            'mean_time': np.mean(ast_times),\n",
    "            'std_time': np.std(ast_times),\n",
    "            'min_time': np.min(ast_times),\n",
    "            'max_time': np.max(ast_times)\n",
    "        }\n",
    "        \n",
    "        print(f\"AST Average: {results['AST']['mean_time']:.3f}s ¬± {results['AST']['std_time']:.3f}s\")\n",
    "    \n",
    "    # VGGish benchmark\n",
    "    if vggish_model is not None:\n",
    "        print(\"üîÑ Benchmarking VGGish model...\")\n",
    "        vggish_times = []\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            # Simulate VGGish prediction (replace with actual prediction call)\n",
    "            # prediction = predict_raga_vggish(vggish_model, sample_audio_path)\n",
    "            time.sleep(0.05)  # Placeholder - replace with actual prediction\n",
    "            end_time = time.time()\n",
    "            vggish_times.append(end_time - start_time)\n",
    "        \n",
    "        results['VGGish'] = {\n",
    "            'mean_time': np.mean(vggish_times),\n",
    "            'std_time': np.std(vggish_times),\n",
    "            'min_time': np.min(vggish_times),\n",
    "            'max_time': np.max(vggish_times)\n",
    "        }\n",
    "        \n",
    "        print(f\"VGGish Average: {results['VGGish']['mean_time']:.3f}s ¬± {results['VGGish']['std_time']:.3f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage instructions\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üìã MODEL COMPARISON USAGE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\\\nüí° After training both models, compare them with:\")\n",
    "print(\"comparison_results = compare_models(ast_eval_results, vggish_eval_results)\")\n",
    "print(\"\\\\n‚è±Ô∏è  Benchmark inference speed with:\")\n",
    "print(\"speed_results = benchmark_inference_speed(ast_model, vggish_model, 'sample_audio.wav')\")\n",
    "print(\"\\\\nüéØ This will generate:\")\n",
    "print(\"   ‚Ä¢ Performance comparison charts\")\n",
    "print(\"   ‚Ä¢ Confusion matrix visualizations\") \n",
    "print(\"   ‚Ä¢ Architecture comparison table\")\n",
    "print(\"   ‚Ä¢ Comprehensive JSON report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecceb920",
   "metadata": {},
   "source": [
    "## 7. Model Deployment and Sharing\n",
    "\n",
    "### HuggingFace Hub Integration\n",
    "Deploy and share trained models on HuggingFace Hub for easy access and reproducibility.\n",
    "\n",
    "**Deployment Features:**\n",
    "- Model cards with detailed documentation\n",
    "- Inference API for real-time predictions\n",
    "- Version control and model tracking\n",
    "- Community sharing and collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23df12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Hub Deployment\n",
    "from huggingface_hub import HfApi, create_repo, upload_folder\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def create_model_card(model_name: str, model_type: str, results: dict, dataset_info: dict):\n",
    "    \"\"\"Create a comprehensive model card for HuggingFace Hub.\"\"\"\n",
    "    \n",
    "    accuracy = results.get('eval_accuracy' if 'eval_accuracy' in results else 'test_accuracy', 0)\n",
    "    f1_macro = results.get('eval_f1_macro', results.get('classification_report', {}).get('macro avg', {}).get('f1-score', 0))\n",
    "    \n",
    "    model_card = f\"\"\"---\n",
    "language:\n",
    "- en\n",
    "- hi\n",
    "tags:\n",
    "- audio-classification\n",
    "- music\n",
    "- indian-classical\n",
    "- raga-classification\n",
    "- {model_type.lower()}\n",
    "datasets:\n",
    "- custom-raga-dataset\n",
    "metrics:\n",
    "- accuracy\n",
    "- f1\n",
    "library_name: {'transformers' if model_type == 'AST' else 'tensorflow'}\n",
    "pipeline_tag: audio-classification\n",
    "---\n",
    "\n",
    "# {model_name} for Indian Classical Raga Classification\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This model is a fine-tuned {model_type} model for classifying Indian classical music ragas. The model was trained on a custom dataset of {dataset_info.get('total_samples', 'N/A')} audio samples across {len(CLASS_NAMES)} different ragas.\n",
    "\n",
    "### Model Architecture\n",
    "- **Base Model**: {model_type}\n",
    "- **Task**: Multi-class audio classification\n",
    "- **Classes**: {len(CLASS_NAMES)} Indian classical ragas\n",
    "- **Input**: {'Raw audio waveform' if model_type == 'AST' else 'Log-mel spectrogram'}\n",
    "\n",
    "### Raga Classes\n",
    "{', '.join(CLASS_NAMES)}\n",
    "\n",
    "## Training Data\n",
    "\n",
    "The model was trained on a diverse dataset of Indian classical music recordings:\n",
    "- **Total Samples**: {dataset_info.get('total_samples', 'N/A')}\n",
    "- **Train/Val/Test Split**: {dataset_info.get('train_split', 'N/A')}/{dataset_info.get('val_split', 'N/A')}/{dataset_info.get('test_split', 'N/A')}\n",
    "- **Audio Format**: {dataset_info.get('audio_format', 'WAV, 22050 Hz')}\n",
    "- **Duration Range**: {dataset_info.get('duration_range', '30-180 seconds')}\n",
    "\n",
    "## Performance\n",
    "\n",
    "| Metric | Score |\n",
    "|--------|--------|\n",
    "| Accuracy | {accuracy:.4f} |\n",
    "| F1 Score (Macro) | {f1_macro:.4f} |\n",
    "\n",
    "### Per-Class Performance\n",
    "\"\"\"\n",
    "\n",
    "    if 'classification_report' in results:\n",
    "        model_card += \"\\\\n| Raga | Precision | Recall | F1-Score |\\\\n|------|-----------|--------|----------|\\\\n\"\n",
    "        for raga in CLASS_NAMES:\n",
    "            if raga in results['classification_report']:\n",
    "                precision = results['classification_report'][raga]['precision']\n",
    "                recall = results['classification_report'][raga]['recall']\n",
    "                f1 = results['classification_report'][raga]['f1-score']\n",
    "                model_card += f\"| {raga} | {precision:.3f} | {recall:.3f} | {f1:.3f} |\\\\n\"\n",
    "\n",
    "    model_card += f\"\"\"\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Using Transformers (for AST)\n",
    "```python\n",
    "from transformers import ASTForAudioClassification, ASTProcessor\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# Load model and processor\n",
    "model = ASTForAudioClassification.from_pretrained(\"your-username/{model_name.lower().replace(' ', '-')}\")\n",
    "processor = ASTProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "# Load and process audio\n",
    "waveform, sr = librosa.load(\"audio_file.wav\", sr=16000)\n",
    "inputs = processor(waveform, sampling_rate=sr, return_tensors=\"pt\")\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1)\n",
    "```\n",
    "\n",
    "### Using TensorFlow (for VGGish)\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model(\"path/to/vggish_model\")\n",
    "\n",
    "# Preprocess audio (implement vggish_preprocess_audio function)\n",
    "spectrogram = vggish_preprocess_audio(audio_waveform)\n",
    "prediction = model.predict(np.expand_dims(spectrogram, axis=0))\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "### Training Configuration\n",
    "- **Epochs**: {CONFIG.get('epochs', 'N/A')}\n",
    "- **Batch Size**: {CONFIG.get('batch_size', 'N/A')}\n",
    "- **Learning Rate**: {CONFIG.get('learning_rate_' + model_type.lower(), 'N/A')}\n",
    "- **Optimizer**: Adam\n",
    "- **Loss Function**: Sparse Categorical Crossentropy\n",
    "\n",
    "### Data Augmentation\n",
    "- Audio time stretching\n",
    "- Pitch shifting  \n",
    "- Background noise addition\n",
    "- Volume normalization\n",
    "\n",
    "## Limitations and Biases\n",
    "\n",
    "- Model trained primarily on specific recording conditions\n",
    "- Performance may vary with different audio qualities\n",
    "- Limited to {len(CLASS_NAMES)} specific ragas\n",
    "- May not generalize to fusion or contemporary adaptations\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{{model_name.lower().replace(' ', '_')}_raga_classification,\n",
    "  title={{{model_name} for Indian Classical Raga Classification}},\n",
    "  author={{Your Name}},\n",
    "  year={{2024}},\n",
    "  howpublished={{\\\\url{{https://huggingface.co/your-username/{model_name.lower().replace(' ', '-')}}}}},\n",
    "}}\n",
    "```\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- MIT Audio Spectrogram Transformer (AST) team\n",
    "- Google VGGish model creators\n",
    "- Indian classical music community\n",
    "- HuggingFace for hosting and infrastructure\n",
    "\"\"\"\n",
    "\n",
    "    return model_card\n",
    "\n",
    "def deploy_ast_to_hub(model, processor, results: dict, repo_name: str, dataset_info: dict):\n",
    "    \"\"\"Deploy AST model to HuggingFace Hub.\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Deploying AST model to HuggingFace Hub: {repo_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create repository\n",
    "        api = HfApi()\n",
    "        \n",
    "        try:\n",
    "            create_repo(repo_name, exist_ok=True)\n",
    "            print(f\"‚úÖ Repository created/found: {repo_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Repository creation note: {e}\")\n",
    "        \n",
    "        # Save model and processor temporarily\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            print(f\"üíæ Saving model to temporary directory: {temp_dir}\")\n",
    "            \n",
    "            # Save model\n",
    "            model.save_pretrained(temp_dir)\n",
    "            processor.save_pretrained(temp_dir)\n",
    "            \n",
    "            # Create model card\n",
    "            model_card = create_model_card(\"Audio Spectrogram Transformer\", \"AST\", results, dataset_info)\n",
    "            with open(f\"{temp_dir}/README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(model_card)\n",
    "            \n",
    "            # Save configuration\n",
    "            config = {\n",
    "                \"model_type\": \"AST\",\n",
    "                \"num_classes\": len(CLASS_NAMES),\n",
    "                \"class_names\": CLASS_NAMES,\n",
    "                \"sample_rate\": CONFIG['sample_rate'],\n",
    "                \"max_length\": CONFIG['max_length']\n",
    "            }\n",
    "            \n",
    "            with open(f\"{temp_dir}/config.json\", \"w\") as f:\n",
    "                json.dump(config, f, indent=2)\n",
    "            \n",
    "            # Upload to Hub\n",
    "            print(\"üîÑ Uploading to HuggingFace Hub...\")\n",
    "            upload_folder(\n",
    "                folder_path=temp_dir,\n",
    "                repo_id=repo_name,\n",
    "                repo_type=\"model\"\n",
    "            )\n",
    "            \n",
    "        print(f\"‚úÖ AST model deployed successfully!\")\n",
    "        print(f\"üåê Access your model at: https://huggingface.co/{repo_name}\")\n",
    "        \n",
    "        return f\"https://huggingface.co/{repo_name}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Deployment failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def deploy_vggish_to_hub(model, results: dict, repo_name: str, dataset_info: dict):\n",
    "    \"\"\"Deploy VGGish model to HuggingFace Hub.\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Deploying VGGish model to HuggingFace Hub: {repo_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create repository\n",
    "        api = HfApi()\n",
    "        \n",
    "        try:\n",
    "            create_repo(repo_name, exist_ok=True)\n",
    "            print(f\"‚úÖ Repository created/found: {repo_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Repository creation note: {e}\")\n",
    "        \n",
    "        # Save model temporarily\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            print(f\"üíæ Saving model to temporary directory: {temp_dir}\")\n",
    "            \n",
    "            # Save TensorFlow model\n",
    "            model.save(f\"{temp_dir}/vggish_model.h5\")\n",
    "            \n",
    "            # Create model card\n",
    "            model_card = create_model_card(\"VGGish\", \"VGGish\", results, dataset_info)\n",
    "            with open(f\"{temp_dir}/README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(model_card)\n",
    "            \n",
    "            # Save configuration\n",
    "            config = {\n",
    "                \"model_type\": \"VGGish\",\n",
    "                \"num_classes\": len(CLASS_NAMES),\n",
    "                \"class_names\": CLASS_NAMES,\n",
    "                \"sample_rate\": VGGISH_CONFIG['sample_rate'],\n",
    "                \"mel_bands\": VGGISH_CONFIG['mel_bands'],\n",
    "                \"embedding_size\": VGGISH_CONFIG['embedding_size']\n",
    "            }\n",
    "            \n",
    "            with open(f\"{temp_dir}/config.json\", \"w\") as f:\n",
    "                json.dump(config, f, indent=2)\n",
    "            \n",
    "            # Create inference script\n",
    "            inference_script = '''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import json\n",
    "\n",
    "def load_model_and_config(model_path):\n",
    "    model = tf.keras.models.load_model(f\"{model_path}/vggish_model.h5\")\n",
    "    with open(f\"{model_path}/config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return model, config\n",
    "\n",
    "def predict(model, config, audio_path):\n",
    "    # Load and preprocess audio\n",
    "    waveform, sr = librosa.load(audio_path, sr=config[\"sample_rate\"])\n",
    "    \n",
    "    # Apply VGGish preprocessing (implement vggish_preprocess_audio)\n",
    "    # spectrograms = vggish_preprocess_audio(waveform, sr)\n",
    "    \n",
    "    # Predict\n",
    "    # predictions = model.predict(spectrograms)\n",
    "    # return predictions\n",
    "    \n",
    "    return {\"message\": \"Implement vggish_preprocess_audio function\"}\n",
    "'''\n",
    "            \n",
    "            with open(f\"{temp_dir}/inference.py\", \"w\") as f:\n",
    "                f.write(inference_script)\n",
    "            \n",
    "            # Upload to Hub\n",
    "            print(\"üîÑ Uploading to HuggingFace Hub...\")\n",
    "            upload_folder(\n",
    "                folder_path=temp_dir,\n",
    "                repo_id=repo_name,\n",
    "                repo_type=\"model\"\n",
    "            )\n",
    "            \n",
    "        print(f\"‚úÖ VGGish model deployed successfully!\")\n",
    "        print(f\"üåê Access your model at: https://huggingface.co/{repo_name}\")\n",
    "        \n",
    "        return f\"https://huggingface.co/{repo_name}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Deployment failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Dataset information for model cards\n",
    "dataset_info = {\n",
    "    'total_samples': 'TBD',  # Will be filled after data loading\n",
    "    'train_split': '70%',\n",
    "    'val_split': '15%', \n",
    "    'test_split': '15%',\n",
    "    'audio_format': 'WAV, 22050 Hz',\n",
    "    'duration_range': '30-180 seconds'\n",
    "}\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üöÄ MODEL DEPLOYMENT READY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\\\nüìã Deployment Instructions:\")\n",
    "print(\"\\\\nüéØ For AST model:\")\n",
    "print(\"ast_hub_url = deploy_ast_to_hub(\")\n",
    "print(\"    model=trained_ast_model,\")\n",
    "print(\"    processor=ast_processor,\") \n",
    "print(\"    results=ast_eval_results,\")\n",
    "print(\"    repo_name='your-username/ast-raga-classifier',\")\n",
    "print(\"    dataset_info=dataset_info\")\n",
    "print(\")\")\n",
    "print(\"\\\\nüéØ For VGGish model:\")\n",
    "print(\"vggish_hub_url = deploy_vggish_to_hub(\")\n",
    "print(\"    model=trained_vggish_model,\")\n",
    "print(\"    results=vggish_eval_results,\")\n",
    "print(\"    repo_name='your-username/vggish-raga-classifier',\")\n",
    "print(\"    dataset_info=dataset_info\")\n",
    "print(\")\")\n",
    "print(\"\\\\n‚ö†Ô∏è  Remember to:\")\n",
    "print(\"   ‚Ä¢ Login to HuggingFace: huggingface-cli login\")\n",
    "print(\"   ‚Ä¢ Replace 'your-username' with your actual username\")\n",
    "print(\"   ‚Ä¢ Ensure you have write permissions to the repositories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f73146",
   "metadata": {},
   "source": [
    "## 8. Complete Execution Workflow\n",
    "\n",
    "### Step-by-Step Execution Guide\n",
    "\n",
    "This section provides a complete workflow to execute the entire notebook from start to finish.\n",
    "\n",
    "**Execution Order:**\n",
    "1. Environment setup and data loading\n",
    "2. AST model training and evaluation  \n",
    "3. VGGish model training and evaluation\n",
    "4. Model comparison and analysis\n",
    "5. Deployment to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Execution Workflow\n",
    "def execute_complete_pipeline():\n",
    "    \"\"\"\n",
    "    Complete execution pipeline for AST and VGGish raga classification.\n",
    "    This function demonstrates the complete workflow but should be executed step by step.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"üéµ COMPLETE RAGA CLASSIFICATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    workflow_steps = [\n",
    "        \"1Ô∏è‚É£  Environment Setup and GPU Configuration\",\n",
    "        \"2Ô∏è‚É£  Data Loading and Preprocessing\", \n",
    "        \"3Ô∏è‚É£  AST Model Setup and Training\",\n",
    "        \"4Ô∏è‚É£  AST Model Evaluation\",\n",
    "        \"5Ô∏è‚É£  VGGish Model Setup and Training\", \n",
    "        \"6Ô∏è‚É£  VGGish Model Evaluation\",\n",
    "        \"7Ô∏è‚É£  Model Comparison and Analysis\",\n",
    "        \"8Ô∏è‚É£  Deployment to HuggingFace Hub\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\\\nüìã Execution Steps:\")\n",
    "    for step in workflow_steps:\n",
    "        print(f\"   {step}\")\n",
    "    \n",
    "    print(\"\\\\n‚ö†Ô∏è  IMPORTANT NOTES:\")\n",
    "    print(\"   ‚Ä¢ Execute cells sequentially, not all at once\")\n",
    "    print(\"   ‚Ä¢ Monitor GPU usage and memory\")\n",
    "    print(\"   ‚Ä¢ Training may take several hours\")\n",
    "    print(\"   ‚Ä¢ Save checkpoints regularly\")\n",
    "    print(\"   ‚Ä¢ Verify data paths before training\")\n",
    "    \n",
    "    execution_checklist = {\n",
    "        \"Environment\": \"‚úÖ Python packages installed, GPU configured\",\n",
    "        \"Data\": \"‚úÖ Audio files loaded, classes defined, train/val/test split\",\n",
    "        \"AST\": \"‚úÖ Model loaded, datasets created, training ready\",\n",
    "        \"VGGish\": \"‚úÖ Model created, preprocessing setup, training ready\", \n",
    "        \"Training\": \"‚ö†Ô∏è  Execute training cells manually\",\n",
    "        \"Evaluation\": \"‚ö†Ô∏è  Run after training completion\",\n",
    "        \"Comparison\": \"‚ö†Ô∏è  Run after both models trained\",\n",
    "        \"Deployment\": \"‚ö†Ô∏è  Configure HuggingFace credentials first\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\\\n‚úÖ Pre-Execution Checklist:\")\n",
    "    for item, status in execution_checklist.items():\n",
    "        print(f\"   {item}: {status}\")\n",
    "    \n",
    "    return workflow_steps\n",
    "\n",
    "# Training Execution Template\n",
    "def training_execution_template():\n",
    "    \"\"\"Template for executing the training pipeline.\"\"\"\n",
    "    \n",
    "    training_code = '''\n",
    "# STEP 1: Ensure all setup is complete\n",
    "print(\"Verifying setup...\")\n",
    "assert 'CLASS_NAMES' in locals(), \"‚ùå Class names not defined\"\n",
    "assert 'train_files' in locals(), \"‚ùå Training files not loaded\"\n",
    "assert 'device' in locals(), \"‚ùå Device not configured\"\n",
    "print(\"‚úÖ Setup verified\")\n",
    "\n",
    "# STEP 2: Train AST Model\n",
    "print(\"\\\\nüöÄ Starting AST training...\")\n",
    "# Uncomment to execute:\n",
    "# ast_train_result = train_ast_model(ast_trainer, ast_output_dir)\n",
    "# ast_eval_results, ast_report, ast_cm = evaluate_ast_model(ast_trainer, ast_test_loader, ast_output_dir)\n",
    "\n",
    "# STEP 3: Train VGGish Model  \n",
    "print(\"\\\\nüöÄ Starting VGGish training...\")\n",
    "# Uncomment to execute:\n",
    "# vggish_history = train_vggish_two_phase(vggish_model, vggish_feature_layer, vggish_train_dataset, vggish_val_dataset, vggish_callbacks)\n",
    "# vggish_eval_results = evaluate_vggish_model(vggish_model, vggish_test_dataset, os.path.join(MODELS_PATH, 'vggish_model'))\n",
    "\n",
    "# STEP 4: Compare Models\n",
    "print(\"\\\\nüìä Comparing models...\")\n",
    "# Uncomment to execute:\n",
    "# comparison_results = compare_models(ast_eval_results, vggish_eval_results)\n",
    "\n",
    "# STEP 5: Deploy to HuggingFace\n",
    "print(\"\\\\nüöÄ Deploying to HuggingFace...\")\n",
    "# Uncomment to execute:\n",
    "# ast_hub_url = deploy_ast_to_hub(ast_model, ast_processor, ast_eval_results, 'your-username/ast-raga-classifier', dataset_info)\n",
    "# vggish_hub_url = deploy_vggish_to_hub(vggish_model, vggish_eval_results, 'your-username/vggish-raga-classifier', dataset_info)\n",
    "\n",
    "print(\"‚úÖ Pipeline template ready - uncomment sections to execute\")\n",
    "'''\n",
    "    \n",
    "    print(\"\\\\nüìù Training Execution Template:\")\n",
    "    print(training_code)\n",
    "    \n",
    "    return training_code\n",
    "\n",
    "# Execute workflow overview\n",
    "workflow_steps = execute_complete_pipeline()\n",
    "training_template = training_execution_template()\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"üìä NOTEBOOK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = {\n",
    "    \"üéØ Objective\": \"Compare AST and VGGish models for Indian classical raga classification\",\n",
    "    \"üèóÔ∏è Models\": \"Audio Spectrogram Transformer (AST) + VGGish CNN\",\n",
    "    \"üìÅ Architecture\": \"Modular design with clear separation between models\",\n",
    "    \"üîß Features\": [\n",
    "        \"Complete data preprocessing pipelines\",\n",
    "        \"Two-phase VGGish training (frozen ‚Üí fine-tuning)\",\n",
    "        \"Comprehensive evaluation metrics\",\n",
    "        \"Visual model comparison\",\n",
    "        \"HuggingFace Hub deployment\",\n",
    "        \"Reproducible experimental setup\"\n",
    "    ],\n",
    "    \"üìà Outputs\": [\n",
    "        \"Trained models with performance metrics\",\n",
    "        \"Confusion matrices and classification reports\", \n",
    "        \"Model comparison visualizations\",\n",
    "        \"Deployable models on HuggingFace Hub\",\n",
    "        \"Complete documentation and model cards\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"\\\\n{key}:\")\n",
    "        for item in value:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "    else:\n",
    "        print(f\"\\\\n{key}: {value}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"üéâ NOTEBOOK SETUP COMPLETE - Ready for Execution!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\\\nüí° Next Steps:\")\n",
    "print(\"   1. Verify your data paths and GPU setup\")\n",
    "print(\"   2. Execute cells sequentially from the beginning\") \n",
    "print(\"   3. Monitor training progress and save checkpoints\")\n",
    "print(\"   4. Compare results and deploy best performing model\")\n",
    "print(\"   5. Share your research and findings!\")\n",
    "\n",
    "# Save notebook execution log\n",
    "execution_log = {\n",
    "    \"notebook_created\": datetime.now().isoformat(),\n",
    "    \"models\": [\"AST\", \"VGGish\"],\n",
    "    \"classes\": CLASS_NAMES if 'CLASS_NAMES' in locals() else [],\n",
    "    \"workflow_steps\": workflow_steps,\n",
    "    \"status\": \"Ready for execution\"\n",
    "}\n",
    "\n",
    "log_file = os.path.join(BASE_PATH, \"execution_log.json\")\n",
    "with open(log_file, 'w') as f:\n",
    "    json.dump(execution_log, f, indent=2)\n",
    "\n",
    "print(f\"\\\\nüìù Execution log saved: {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
